{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/diogoflim/AM/blob/main/2_Gradiente_Descendente_RegL.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Aprendizado de Máquina\n",
    "\n",
    "**Professor: Diogo Ferreira de Lima Silva (TEP)**\n",
    "\n",
    "**PPGEP - UFF**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook foi criado é com base em:\n",
    "\n",
    "- Raschka, S., Liu, Y. H., Mirjalili, V., & Dzhulgakov, D. (2022). Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd.\n",
    "\n",
    "- Veja também o código do capítulo 9: https://github.com/rasbt/machine-learning-book\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bibliotecas Básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente Descendente em Regressão Linear\n",
    "\n",
    "Nessa aula, vamos implementar o algoritmo gradiente descendente para regressão linear.\n",
    "\n",
    "Para isso, utilizaremos os conceitos de Classes em Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando o Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos criar uma classe vazia, sem nenhum método ou construtor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em python, é comum que iniciemos as classes com um método especial, chamado usando o operador: __init__\n",
    "\n",
    "Esse será o construtor de nossa classe. Assim, os parâmetros obtigatórios passados na criação de um objeto são aqui definidos. \n",
    "\n",
    "Na construção da clase, um parâmetro especial chamado de **self** é sempre criado para se referir ao próprio objeto nas operações posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state  \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, já poderíamos criar um objeto dessa classe. Os parâmetros definidos foram:\n",
    "\n",
    "- eta=0.01: a taxa de aprendizado\n",
    "- n_iter=50: o número de iterações do algoritmo\n",
    "- random_state=1: a semente aleatória que será utilizada na definição inicial de **w**\n",
    "\n",
    "Os valores definidos dentro dos parênteses são usados como default caso nenhum parâmetro seja passado na criação do objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.005\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegressionGD()\n",
    "print(lr.eta)\n",
    "\n",
    "\n",
    "lr = LinearRegressionGD(eta = .005, n_iter = 20)\n",
    "print(lr.eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Já temos uma classe. Porém, nada ainda pode ser feito. Para isso criaremos os métodos dessa classe. São funções criadas dentro das classes.\n",
    "\n",
    "Iniciaremos com a função de aprendizado. Seguindo o padrão do sklearn, chamaremos esse método de fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state  \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state) # cria um gerador de números aleatórios\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # Cria o vetor inicial de pesos\n",
    "        self.b_ = np.array([0.]) # cria o bias inicial\n",
    "        self.losses_ = [] # Uma lista vazia para alocarmos as perdas calculadas em cada passo do algoritmo\n",
    "        \n",
    "        # Loop no número de iterações\n",
    "        for i in range(self.n_iter):\n",
    "            output = np.dot(X, self.w_) + self.b_ # Realiza as previsões dado w e b de momento\n",
    "            desvios = (output -  y) # calcula o desvio de cada exemplo\n",
    "            self.w_ -= self.eta * X.T.dot(desvios) / X.shape[0] # Atualiza o w\n",
    "            self.b_ -= self.eta * desvios.mean() # Atualiza o b\n",
    "            loss = (desvios**2).mean() # Computa a perda\n",
    "            self.losses_.append(loss) # Coloca a perda visualizada no passo na lista de perdas\n",
    "        return self # Retorna o objeto transformado (temos novos w_ e b_)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, já somos capazes de treinar o nosso modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rgen = np.random.RandomState(42)\n",
    "\n",
    "\n",
    "# Vamos gerar dados genéricos\n",
    "def generate_data(num_samples=1000):\n",
    "    area = data_rgen.uniform(500, 3500, num_samples) \n",
    "    quartos = data_rgen.randint(1, 6, num_samples)        \n",
    "    idade = data_rgen.randint(0, 80, num_samples)            \n",
    "    \n",
    "    # Target variable (house price)\n",
    "    base = 50000\n",
    "    base_area = 100  # Preço adicional por m^2\n",
    "    base_quarto = 20000  # Preço adicional por quarto\n",
    "    base_idade = 500  # Redução no preço por ano\n",
    "    noise = data_rgen.normal(0, 10000, num_samples)  # Random noise\n",
    "    \n",
    "    price = (base + area * base_area + quartos * base_quarto - idade * base_idade + 0.05 * area**2 + 0.1 * area * idade + 50 * quartos**2 - 0.05 * idade*2 + noise)\n",
    "    X=np.array([area,quartos,idade]).T\n",
    "    y = price\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1623.62036,  4.00000,  33.00000],\n",
       "       [ 3352.14292,  3.00000,  7.00000],\n",
       "       [ 2695.98183,  5.00000,  58.00000],\n",
       "       ...,\n",
       "       [ 910.45589,  5.00000,  31.00000],\n",
       "       [ 3350.71206,  4.00000,  61.00000],\n",
       "       [ 1838.01732,  3.00000,  73.00000]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 431828.94775,  1018742.66304,  772991.57144,  539100.67306,\n",
       "        283625.29986,  172504.97460,  178196.57221,  853776.88695,\n",
       "        625136.83832,  728632.18993,  152316.19253,  980403.63890,\n",
       "        887102.28154,  248146.60695,  212389.25660,  198075.36752,\n",
       "        310950.88169,  575330.13360,  406784.65895,  385837.08627,\n",
       "        570219.14240,  237853.54633,  317843.14130,  393300.54893,\n",
       "        514566.81918,  762554.27007,  282807.42980,  517356.61626,\n",
       "        549866.44929,  180524.73724,  585825.79154,  220343.60741,\n",
       "        185223.87342,  1017977.32683,  1055302.10343,  869402.58818,\n",
       "        332925.13031,  193652.86556,  732783.69015,  440603.42627,\n",
       "        187036.61323,  517889.76922,  131120.06459,  972320.19993,\n",
       "        340937.80363,  628251.61253,  375095.54719,  496299.78210,\n",
       "        496881.52298,  282709.67736,  1005952.95657,  815820.11100,\n",
       "        961013.00912,  929807.74307,  577213.64302,  949915.10006,\n",
       "        197197.91833,  295873.35048,  224279.00820,  316189.39860,\n",
       "        400416.49316,  321182.46542,  864516.19045,  435451.19954,\n",
       "        294095.66996,  590915.92785,  286615.29116,  829133.25384,\n",
       "        162308.25937,  1072298.52352,  743332.81029,  221058.10802,\n",
       "        108748.44649,  855759.57182,  742111.89168,  685876.50821,\n",
       "        732268.25367,  201592.49183,  410370.04554,  206542.37974,\n",
       "        899314.11624,  609131.11480,  397764.25434,  152621.74588,\n",
       "        306645.54840,  389822.38251,  696897.59248,  587168.12056,\n",
       "        891212.64625,  494839.99120,  248368.11208,  702264.12206,\n",
       "        728555.99897,  599981.13241,  836095.13325,  547188.21445,\n",
       "        524790.61116,  456309.18959,  179594.33422,  272592.85274,\n",
       "        143140.71774,  592120.30084,  368237.40843,  537992.78623,\n",
       "        947680.60489,  304369.28484,  473743.61960,  806944.34870,\n",
       "        266934.63577,  236354.47306,  306633.54208,  258223.90724,\n",
       "        937353.40669,  763983.75095,  619399.28454,  855268.71169,\n",
       "        782823.84370,  246088.99496,  925761.92696,  593892.54501,\n",
       "        846571.86969,  884496.40009,  317642.91798,  187717.80758,\n",
       "        311765.98458,  420742.98083,  801982.18032,  936003.95985,\n",
       "        162602.53182,  474760.88348,  434462.73301,  247512.24712,\n",
       "        222194.05757,  334370.23055,  978704.98757,  358681.81093,\n",
       "        554162.58354,  754348.29485,  367398.51398,  983394.65964,\n",
       "        1029482.06220,  321653.47781,  469437.81478,  349788.11238,\n",
       "        279034.03131,  183694.46627,  602442.76673,  519168.52332,\n",
       "        221498.06575,  286654.58104,  934154.84873,  241927.74248,\n",
       "        225221.55580,  504139.48482,  1031057.16737,  263133.80721,\n",
       "        631781.13947,  829241.78191,  274467.89578,  748825.38918,\n",
       "        352600.29075,  641284.26171,  629157.05639,  568784.83656,\n",
       "        165366.17861,  838208.54998,  423587.46007,  256002.18689,\n",
       "        210223.47323,  597696.90115,  724815.11075,  214407.84260,\n",
       "        453831.64653,  281440.51917,  706097.67893,  303191.91820,\n",
       "        686714.61703,  362059.55331,  970642.13929,  226592.36103,\n",
       "        385618.89120,  260433.61392,  1006499.71186,  870357.40903,\n",
       "        295278.37727,  677474.31505,  797565.70968,  551877.31981,\n",
       "        518852.88793,  246956.20459,  246314.15201,  934122.50601,\n",
       "        991275.95485,  587798.33628,  394595.25353,  347885.07384,\n",
       "        749941.47900,  933743.03898,  931463.42497,  842089.63333,\n",
       "        647647.86908,  225852.20619,  211695.85682,  899111.26302,\n",
       "        592151.19159,  131679.76736,  159793.69134,  667511.16176,\n",
       "        189572.91348,  237578.19621,  556173.77255,  669209.38451,\n",
       "        689500.55058,  326558.21572,  739769.00636,  264489.58357,\n",
       "        411378.41945,  726577.20571,  612081.68727,  838712.61070,\n",
       "        610300.34132,  521461.03163,  212191.32039,  446227.25548,\n",
       "        304508.68218,  323892.00834,  990057.32623,  355939.92903,\n",
       "        890558.88918,  655506.23346,  835432.29324,  539242.39362,\n",
       "        584616.45340,  518521.98182,  276056.80791,  742616.12088,\n",
       "        313709.03031,  172543.06781,  652571.04614,  209311.99148,\n",
       "        986146.91136,  973849.57071,  923867.55379,  358946.88973,\n",
       "        146312.15585,  967785.42600,  421129.53514,  1004843.50363,\n",
       "        1045343.30303,  880327.70840,  357140.18848,  383758.90866,\n",
       "        879855.60738,  364792.23555,  262135.29637,  560746.02344,\n",
       "        1036256.72790,  688807.92383,  520902.00814,  213342.03855,\n",
       "        621633.65697,  1032362.39393,  216144.84523,  511055.13747,\n",
       "        901217.50108,  800854.32138,  724265.23181,  643743.90308,\n",
       "        379438.03368,  370692.55825,  779971.80252,  873357.13131,\n",
       "        940584.45974,  931499.97964,  538530.84732,  465104.61242,\n",
       "        823672.71356,  652937.45459,  714485.47744,  836777.82677,\n",
       "        889709.76450,  392881.97240,  407853.20173,  262986.42359,\n",
       "        612677.85717,  215947.98642,  476423.50597,  570953.66378,\n",
       "        328385.98776,  540629.10564,  227464.13834,  166217.13503,\n",
       "        836956.49604,  337569.47398,  198466.22786,  548267.56947,\n",
       "        758086.54963,  340867.73503,  609891.56483,  180072.66968,\n",
       "        130446.27849,  541637.27890,  529287.66258,  668629.49115,\n",
       "        720058.95591,  1053952.94464,  460214.42496,  303938.07850,\n",
       "        850858.64975,  348564.58925,  452017.88086,  181462.08921,\n",
       "        178517.97659,  1006871.63330,  906036.32026,  714914.62411,\n",
       "        464206.75040,  263611.54206,  197255.46903,  306709.20766,\n",
       "        566840.78070,  659016.94291,  615320.05364,  327181.80204,\n",
       "        1031938.45668,  775630.08516,  526850.01007,  556957.54151,\n",
       "        455312.04457,  331032.49430,  402205.88224,  750272.26910,\n",
       "        139468.84752,  260351.43049,  111187.34266,  202220.36107,\n",
       "        844591.34828,  689182.23696,  463358.78286,  273633.08697,\n",
       "        438113.14511,  410318.07519,  254464.40347,  404687.11862,\n",
       "        411933.85323,  613679.70581,  596255.66247,  150584.56860,\n",
       "        427757.33432,  587360.32402,  525841.14480,  880502.69558,\n",
       "        616789.87293,  230138.01817,  160331.07806,  643862.97376,\n",
       "        136296.29249,  563363.16914,  943378.46919,  514920.41140,\n",
       "        364719.03434,  679692.01918,  419967.54164,  591278.16147,\n",
       "        990483.39625,  382563.59348,  979873.51859,  913351.21756,\n",
       "        248612.02825,  150545.78985,  175767.39781,  143006.26140,\n",
       "        213205.60481,  638260.80665,  195760.60287,  400944.21566,\n",
       "        854538.98939,  177847.70339,  809574.37971,  293916.46779,\n",
       "        164498.90767,  675861.65513,  648327.81775,  881065.61657,\n",
       "        764857.37430,  857830.68024,  307256.67212,  241519.32914,\n",
       "        781523.36143,  837677.19830,  1043754.83997,  418071.53836,\n",
       "        404512.98564,  722781.88674,  408705.22618,  972734.09047,\n",
       "        883527.45399,  415594.01693,  762399.37993,  794187.56787,\n",
       "        267727.71506,  899088.32708,  553202.44159,  848272.97364,\n",
       "        324030.79267,  923355.66862,  426036.78556,  213361.46556,\n",
       "        990334.06458,  250656.56111,  345265.18468,  1032790.11930,\n",
       "        982749.85026,  525414.97856,  649209.82600,  510710.12550,\n",
       "        285914.22958,  364755.77499,  673335.00733,  783175.28873,\n",
       "        781489.30844,  800964.07291,  181152.24317,  532094.29975,\n",
       "        231114.71299,  488300.50213,  471420.69497,  875975.71843,\n",
       "        422516.64065,  216045.23042,  279867.89404,  786029.31965,\n",
       "        666815.01116,  227558.43834,  232966.55532,  723125.16243,\n",
       "        214832.68997,  852679.26331,  688165.92383,  161425.80368,\n",
       "        176158.25190,  1014664.91847,  406254.12377,  369386.53520,\n",
       "        883403.79459,  1001896.53856,  1057217.48190,  726149.37842,\n",
       "        377296.52128,  219550.97489,  764728.70092,  567778.25777,\n",
       "        415306.11721,  928956.97216,  239540.23181,  519009.72533,\n",
       "        173027.44227,  509010.94207,  153606.60537,  273463.35781,\n",
       "        181143.61894,  636940.92130,  736873.54736,  585911.56379,\n",
       "        996649.23521,  407527.61874,  307607.54330,  952310.75956,\n",
       "        299404.47179,  1066556.94555,  123242.11462,  1042926.82047,\n",
       "        225302.15243,  881715.89865,  540773.58026,  1050014.59305,\n",
       "        164896.04013,  533941.61265,  970803.79543,  463474.77145,\n",
       "        683500.68075,  682305.11563,  449426.44195,  648105.37272,\n",
       "        624982.73665,  906465.80828,  193108.23025,  345390.79778,\n",
       "        1037418.83325,  923986.73523,  437931.71934,  599437.87990,\n",
       "        282330.25696,  299517.14593,  448056.05306,  435594.34666,\n",
       "        531264.33194,  133825.68348,  1087907.12922,  1076203.73944,\n",
       "        696709.33055,  556924.36304,  322298.22488,  782789.42544,\n",
       "        699778.57279,  244633.52243,  915415.16772,  834700.17751,\n",
       "        1038150.59597,  756480.26246,  642034.38595,  477200.71113,\n",
       "        929099.07548,  855601.06787,  198305.50828,  172110.01156,\n",
       "        407456.39506,  829848.81888,  1114602.18943,  268209.00897,\n",
       "        545554.75847,  374670.69148,  995554.05461,  918889.36684,\n",
       "        855653.53479,  424477.14595,  473115.17784,  286492.06447,\n",
       "        171139.61651,  942227.63204,  842931.10071,  1064793.04557,\n",
       "        1103426.49473,  520642.47264,  726002.37018,  995780.03987,\n",
       "        889680.74310,  270410.55001,  468358.92940,  254318.73701,\n",
       "        979530.32050,  597741.08416,  288110.29104,  641232.70844,\n",
       "        562648.28525,  343407.50125,  239782.85465,  620552.56375,\n",
       "        553062.38912,  757319.26574,  552009.84045,  903046.19245,\n",
       "        597717.04368,  587710.49047,  887955.98429,  401875.57319,\n",
       "        256648.83318,  163250.87557,  818895.39287,  561952.84853,\n",
       "        728766.24022,  320239.03161,  186610.68718,  117605.42988,\n",
       "        371595.46827,  572415.00750,  383722.16058,  426063.43984,\n",
       "        923054.67432,  300635.50687,  494444.37968,  749251.83818,\n",
       "        443247.15818,  605253.99344,  835160.43044,  1025225.90398,\n",
       "        183438.23052,  917259.02214,  474454.29524,  338376.08077,\n",
       "        485749.23737,  976693.66995,  478644.99790,  397701.26138,\n",
       "        620102.65798,  265888.72709,  145469.05128,  189314.99618,\n",
       "        195302.92805,  197746.65010,  262440.61406,  601688.87411,\n",
       "        303979.99903,  420307.15181,  904509.91289,  478583.91996,\n",
       "        677585.96154,  246953.98655,  319839.22658,  194636.98021,\n",
       "        245890.59199,  303843.33742,  215822.20550,  224885.07270,\n",
       "        182691.74860,  463634.19249,  327731.21745,  375498.01150,\n",
       "        479064.60420,  667218.45538,  223522.91620,  785908.99888,\n",
       "        627242.64785,  191797.58237,  967426.13790,  962245.01614,\n",
       "        210965.03910,  342831.25804,  765265.05363,  704571.63388,\n",
       "        222069.31266,  219255.70104,  385649.89677,  509240.37976,\n",
       "        581142.68723,  411324.63920,  463790.43167,  739947.27104,\n",
       "        143475.36482,  260531.35820,  710375.74303,  978498.83513,\n",
       "        496854.11592,  514061.01684,  214878.56402,  476314.68207,\n",
       "        569347.36670,  258784.60291,  288008.26344,  401332.06717,\n",
       "        210129.29022,  317852.08636,  328357.56981,  388157.63399,\n",
       "        225971.00881,  925764.39367,  570642.58112,  687734.66456,\n",
       "        789713.62513,  478001.25522,  254672.15457,  551412.47651,\n",
       "        590213.59247,  792542.85874,  474980.68593,  162966.44231,\n",
       "        337519.09859,  387966.96602,  654707.42608,  589040.54572,\n",
       "        429569.52006,  1068710.78507,  599195.86846,  263997.02271,\n",
       "        228873.21818,  265713.80545,  328681.98588,  263019.31798,\n",
       "        258565.30971,  368141.30002,  292875.45130,  920911.26227,\n",
       "        178751.39529,  557930.29660,  401997.03359,  1032502.12008,\n",
       "        189580.46930,  468629.56355,  1044645.76883,  876965.24786,\n",
       "        795545.04825,  306131.02298,  272647.42033,  663674.58263,\n",
       "        1005787.58565,  516679.85776,  558057.20484,  346378.01132,\n",
       "        760770.45071,  271818.84864,  370470.68198,  466017.85178,\n",
       "        449261.21159,  305533.46159,  270120.07118,  570455.49330,\n",
       "        302106.26802,  639669.31838,  308312.33991,  498735.21301,\n",
       "        489069.78339,  174130.89458,  318156.97556,  215644.28249,\n",
       "        165589.81783,  1031910.90495,  410845.04561,  795834.74157,\n",
       "        295440.32853,  659382.46045,  763117.80463,  644592.88243,\n",
       "        508778.64904,  419301.96850,  377113.51160,  994072.55329,\n",
       "        867132.38486,  1004552.08241,  137941.31472,  704918.33389,\n",
       "        985740.99331,  268627.65794,  179013.01797,  705937.93001,\n",
       "        566999.32047,  921873.32011,  191417.49082,  802703.90727,\n",
       "        294963.52698,  219845.93736,  202926.23101,  856502.80605,\n",
       "        606673.77670,  569973.16995,  378456.10562,  920468.30576,\n",
       "        405873.58883,  800391.62942,  410874.41456,  441025.91280,\n",
       "        493671.34772,  299202.86735,  701323.16251,  484102.77006,\n",
       "        271500.07144,  966777.56179,  379618.00778,  518941.06335,\n",
       "        919761.10431,  661116.16939,  172970.47682,  944988.50845,\n",
       "        617710.49471,  389834.77408,  238396.66143,  826556.52878,\n",
       "        575028.09420,  499956.15855,  960693.13830,  810832.70103,\n",
       "        258761.86402,  287187.71980,  321209.47641,  754510.84664,\n",
       "        141774.75362,  509197.82557,  760470.51046,  860570.45001,\n",
       "        387767.19432,  869783.74501,  189404.17652,  869345.64634,\n",
       "        198549.03704,  425269.51999,  846085.24036,  294971.16130,\n",
       "        290677.70791,  752494.11867,  696190.57811,  609353.92242,\n",
       "        705992.21226,  573574.37856,  302571.04315,  359822.19940,\n",
       "        221189.08566,  961974.88414,  569815.73218,  360257.80418,\n",
       "        472179.52409,  983624.33701,  276499.00872,  608584.41680,\n",
       "        491696.72012,  605422.13924,  191244.32169,  947504.24802,\n",
       "        1015341.51568,  515124.21576,  670079.81726,  945241.17282,\n",
       "        742210.29735,  194465.33127,  601613.29246,  617311.61212,\n",
       "        416741.89092,  778816.96299,  1010886.89456,  969289.28184,\n",
       "        482990.07022,  235377.42301,  1080227.16899,  831492.25451,\n",
       "        198845.10671,  1005334.23386,  924261.71122,  472419.46345,\n",
       "        552171.49765,  441370.47139,  200643.73740,  379531.97940,\n",
       "        791248.47971,  184138.50064,  369843.12611,  377015.62515,\n",
       "        507158.51461,  1011380.38351,  334177.25597,  329483.15929,\n",
       "        797986.80159,  429570.90092,  285717.09280,  404895.55237,\n",
       "        216661.22634,  203240.38424,  496756.48903,  441386.48992,\n",
       "        968825.63190,  364619.96035,  605923.32940,  608042.32684,\n",
       "        162502.71670,  651764.44163,  290251.09646,  1069798.68726,\n",
       "        262148.41827,  416014.24230,  199257.80659,  1019769.78028,\n",
       "        473002.27380,  568329.07216,  213805.23129,  746947.81255,\n",
       "        252098.70247,  936139.29727,  274396.38617,  209586.34900,\n",
       "        222973.30355,  473271.83605,  517641.63362,  182249.02104,\n",
       "        757289.45969,  479819.95204,  488366.70121,  477809.49674,\n",
       "        400793.23326,  517116.07197,  201969.16058,  231444.05427,\n",
       "        846421.78367,  1072596.51597,  317390.06028,  353564.22339,\n",
       "        590435.34617,  468527.74175,  198363.97046,  239464.33089,\n",
       "        702703.66180,  611839.56627,  107803.28631,  323373.06857,\n",
       "        308279.84495,  634472.36889,  197861.84621,  215741.12036,\n",
       "        834587.72114,  217342.57802,  659510.07249,  326777.98478,\n",
       "        219514.57156,  331228.76872,  734742.94991,  931516.32011,\n",
       "        865584.76452,  403135.98362,  691573.46065,  289362.26542,\n",
       "        357763.93564,  935022.90031,  139019.28948,  212124.66539,\n",
       "        273152.01347,  152424.31784,  317681.53730,  586587.18078,\n",
       "        409589.59312,  898773.30762,  790359.68160,  354398.66314,\n",
       "        305283.04888,  396439.29105,  608576.66163,  255493.23572,\n",
       "        595639.40724,  375197.52768,  561934.00249,  480079.93215,\n",
       "        290587.11237,  1018201.25481,  769828.36899,  234937.24293,\n",
       "        857174.13833,  484178.13584,  952808.05919,  758822.34087,\n",
       "        475941.44174,  198555.03667,  349138.92634,  585728.64644,\n",
       "        632815.01028,  343705.13736,  264272.16463,  832565.52539,\n",
       "        1104158.62681,  539061.72117,  281674.27304,  318349.58211,\n",
       "        172976.14543,  969639.68355,  260736.50502,  600206.48679,\n",
       "        374397.71779,  524642.92281,  630181.06478,  886570.00779,\n",
       "        282624.64184,  147186.56362,  205775.06273,  997203.13343,\n",
       "        870114.91862,  623392.06314,  658089.04400,  635259.46316,\n",
       "        221477.49997,  991020.49753,  483103.93796,  460586.62230,\n",
       "        497340.84497,  205627.75664,  266861.75347,  753380.02578,\n",
       "        152955.88238,  592859.52119,  253095.91663,  439132.54135,\n",
       "        341354.77196,  431255.92121,  742518.43660,  379644.82130,\n",
       "        525898.76311,  490304.76246,  635821.37462,  995369.64622,\n",
       "        755128.99245,  257091.75814,  153797.32280,  258803.07295,\n",
       "        618827.94113,  233465.86328,  530359.45591,  553587.54431,\n",
       "        410198.15646,  740196.01382,  261770.97894,  136827.55948,\n",
       "        694208.77353,  470517.17322,  630047.35682,  395498.96542,\n",
       "        257392.61679,  829919.05705,  814488.95889,  710862.85082,\n",
       "        263198.50055,  574035.97694,  375522.60562,  170232.82323,\n",
       "        976787.80056,  282600.40652,  998470.59867,  436192.13379])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo os exemplos em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-135874320354987630214561438342748186174722557877735153523935928179604525555021636006727341132646586689174519478453659245565270529627202176107838637446817297070284320636923741380358739423397869708342992287008753544390709057276153648119808.00000\n",
      " -173795149312770193475818666787458725277815230256277644244813985122693241634595010438112331313498679490761637200449932679316141867455919292770897291300880155493726438520223570515773181453714141502749246076258215294002609030966752051200.00000\n",
      " -2208218230097371020344726741312856295389394662200260140548149787484635868075400182851468617758545018287612341843789520340200629834718447294591411261881702011863384000058757279662964169330000856271013017290506693353638289147845408718848.00000] [-57207886221387162043863921306707893260422291999857673454561997254760418890923370309654913033336115625183402007079987827378509496010060875381943625293876718466275573101921266440452643340637739733774203439868437871467223431092113506304.00000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diogo\\AppData\\Local\\Temp/ipykernel_9592/532205798.py:19: RuntimeWarning: overflow encountered in square\n",
      "  loss = (desvios**2).mean() # Computa a perda\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegressionGD()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.w_, lr.b_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[358384055069.3516,\n",
       " 7.918585560026521e+20,\n",
       " 1.7637470943683852e+30,\n",
       " 3.928484183700048e+39,\n",
       " 8.750113908540921e+48,\n",
       " 1.948957659804778e+58,\n",
       " 4.3410131564162725e+67,\n",
       " 9.668960805473204e+76,\n",
       " 2.1536171324335932e+86,\n",
       " 4.7968616756477785e+95,\n",
       " 1.0684295545744089e+105,\n",
       " 2.379767836298747e+114,\n",
       " 5.300578714277424e+123,\n",
       " 1.1806250289502543e+133,\n",
       " 2.6296665592934257e+142,\n",
       " 5.857190931497264e+151,\n",
       " 1.3046021172065167e+161,\n",
       " 2.9058070739460243e+170,\n",
       " 6.472252834507802e+179,\n",
       " 1.4415980031636617e+189,\n",
       " 3.210945023107241e+198,\n",
       " 7.151902207682699e+207,\n",
       " 1.5929797869525307e+217,\n",
       " 3.5481254188758517e+226,\n",
       " 7.902921362333702e+235,\n",
       " 1.7602581274880164e+245,\n",
       " 3.920713029178663e+254,\n",
       " 8.732804818295609e+263,\n",
       " 1.945102317534898e+273,\n",
       " 4.332425955236281e+282,\n",
       " 9.64983409273442e+291,\n",
       " 2.1493569418019267e+301,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf,\n",
       " inf]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.losses_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O modelo não está convergindo**. Provavelmente, a taxa de aprendizado precisaria ser bem menor. \n",
    "\n",
    "\n",
    "Testem com outros valores!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos também transformar os dados para ajudar na convergência do algoritmo. \n",
    "\n",
    "Vamos utilizar um procedimento chamado StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "sc_x = StandardScaler()\n",
    "\n",
    "X_train_std = sc_x.fit_transform(X_train)\n",
    "y_train_std = sc_y.fit_transform(y_train[:, np.newaxis]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39914  0.03259 -0.00672] [-0.00000]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegressionGD()\n",
    "lr.fit(X_train_std, y_train_std)\n",
    "\n",
    "print(lr.w_, lr.b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9693710990528419,\n",
       " 0.9505233485683637,\n",
       " 0.9320489313914756,\n",
       " 0.9139404391808563,\n",
       " 0.8961906108818778,\n",
       " 0.8787923297926471,\n",
       " 0.8617386206886125,\n",
       " 0.8450226470045599,\n",
       " 0.8286377080728514,\n",
       " 0.8125772364167848,\n",
       " 0.7968347950979662,\n",
       " 0.7814040751166204,\n",
       " 0.7662788928637779,\n",
       " 0.7514531876243019,\n",
       " 0.7369210191297385,\n",
       " 0.7226765651599937,\n",
       " 0.7087141191928619,\n",
       " 0.6950280881004487,\n",
       " 0.681612989891549,\n",
       " 0.6684634514990654,\n",
       " 0.6555742066115615,\n",
       " 0.6429400935480732,\n",
       " 0.6305560531753083,\n",
       " 0.6184171268663913,\n",
       " 0.6065184545003206,\n",
       " 0.5948552725013249,\n",
       " 0.5834229119173224,\n",
       " 0.5722167965366993,\n",
       " 0.5612324410426445,\n",
       " 0.5504654492042873,\n",
       " 0.5399115121039056,\n",
       " 0.5295664063994809,\n",
       " 0.5194259926218958,\n",
       " 0.5094862135060816,\n",
       " 0.4997430923554378,\n",
       " 0.49019273143885683,\n",
       " 0.48083131041970695,\n",
       " 0.47165508481612994,\n",
       " 0.46266038449203184,\n",
       " 0.453843612178152,\n",
       " 0.44520124202260963,\n",
       " 0.4367298181703396,\n",
       " 0.428425953370841,\n",
       " 0.420286327613671,\n",
       " 0.4123076867911334,\n",
       " 0.4044868413876142,\n",
       " 0.39682066519503734,\n",
       " 0.3893060940539144,\n",
       " 0.38194012461948,\n",
       " 0.37471981315241154]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.losses_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora o algoritmo parece estar convergindo. Vamos treinar por mais tempo e plotar um gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LinearRegressionGD(n_iter= 500)\n",
    "lr.fit(X_train_std, y_train_std)\n",
    "\n",
    "\n",
    "plt.plot(range(1, lr.n_iter+1), lr.losses_)\n",
    "plt.ylabel('Erro Médio Quadrático')\n",
    "plt.xlabel('Época')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsão\n",
    "\n",
    "\n",
    "Vamos usar nosso conjunto de teste para aplicar os valores que alcançamos no trenamento.\n",
    "\n",
    "Precisamos incorporar mais um método à nossa classe. Mantendo a tradição das bibliotecas de ML, chamaremos de **predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state  \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state) # cria um gerador de números aleatórios\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # Cria o vetor inicial de pesos\n",
    "        self.b_ = np.array([0.]) # cria o bias inicial\n",
    "        self.losses_ = [] # Uma lista vazia para alocarmos as perdas calculadas em cada passo do algoritmo\n",
    "        \n",
    "        # Loop no número de iterações\n",
    "        for i in range(self.n_iter):\n",
    "            output = np.dot(X, self.w_) + self.b_ # Realiza as previsões dado w e b de momento\n",
    "            desvios = (output -  y) # calcula o desvio de cada exemplo\n",
    "            self.w_ -= self.eta * X.T.dot(desvios) / X.shape[0] # Atualiza o w\n",
    "            self.b_ -= self.eta * desvios.mean() # Atualiza o b\n",
    "            loss = (desvios**2).mean() # Computa a perda\n",
    "            self.losses_.append(loss) # Coloca a perda visualizada no passo na lista de perdas\n",
    "        return self # Retorna o objeto transformado (temos novos w_ e b_)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.44900,  0.95617, -0.01834, -0.28932,  1.63059,  1.62359,\n",
       "       -0.14612,  1.10457, -1.44826,  0.18860,  0.96587,  0.78086,\n",
       "        0.24074, -1.00993, -1.14786, -1.33407,  1.17478, -1.34274,\n",
       "       -0.78501, -0.84086,  0.90137,  0.68747, -1.65244, -0.87315,\n",
       "        0.20664,  0.79796,  0.25788,  0.46975, -0.87947,  1.51341,\n",
       "        0.33709,  1.48367, -1.11107,  1.74174, -1.41014,  1.37322,\n",
       "       -0.57634,  0.41092, -1.63590, -0.67140, -0.20773,  0.11202,\n",
       "       -1.53584, -1.55397, -1.48084,  0.54021, -0.13247, -0.81663,\n",
       "       -1.55770, -0.54420,  0.02482, -0.78820,  1.59130, -0.06853,\n",
       "       -0.40957,  0.34150, -0.62135, -1.66326,  0.12050,  0.95765,\n",
       "        0.81283,  0.28896, -0.91907, -1.17131, -1.40718, -0.16692,\n",
       "        0.31846, -1.31130,  0.74808,  0.48170,  1.01443, -0.84984,\n",
       "       -1.04545, -0.81105,  1.11204,  0.11655,  0.34291, -0.18803,\n",
       "       -1.32682, -0.75165,  1.11877, -1.00525, -0.08478,  1.13320,\n",
       "       -1.21769,  0.12408,  1.56485, -0.48987, -0.44999,  0.25726,\n",
       "        0.49730,  0.20427,  1.42550,  0.30800, -0.51490, -1.35231,\n",
       "       -1.69212,  1.19425, -0.30586, -0.59802,  1.17374,  1.22590,\n",
       "       -1.55939,  1.63894, -0.04792, -1.20476,  1.18499,  0.89348,\n",
       "       -1.40172,  1.60501, -1.48822, -0.43162, -0.14412,  0.12091,\n",
       "        0.65842,  0.60427, -0.78908,  0.09609,  0.75879, -1.34217,\n",
       "        1.16086, -0.01767, -0.32212,  0.41864, -1.52201,  0.22550,\n",
       "       -0.45189,  1.45061, -1.15094,  0.79596, -1.42756,  0.29495,\n",
       "        1.00811, -1.45829,  1.42386,  1.35397,  1.19568, -0.99134,\n",
       "        0.86375, -1.78030, -0.70224,  1.52147, -0.96109, -0.40935,\n",
       "       -0.40877,  0.20746,  1.65929, -1.69055, -0.48787,  0.53310,\n",
       "       -0.46475, -0.84709, -1.37197, -1.33100,  0.12069,  0.80027,\n",
       "       -0.65028,  0.47880, -0.35203,  1.13997, -0.09914,  1.15200,\n",
       "        0.00604, -1.18350,  0.15343, -1.04866,  0.38019, -0.90844,\n",
       "        1.30727, -0.18478,  0.69701, -0.10488, -0.77631,  1.03377,\n",
       "       -1.47132,  0.31030, -0.97432, -1.32686,  0.34665, -0.14781,\n",
       "       -0.18711, -0.96294, -1.54302, -0.17022, -0.31999,  0.84256,\n",
       "        0.53817, -0.39979,  0.52491,  0.60284,  1.10198, -0.23283,\n",
       "       -0.64744,  1.09458, -0.77185,  1.56312, -1.63480, -1.48690,\n",
       "       -1.42923, -0.30012, -1.47537,  1.51602,  0.25376, -1.64008,\n",
       "        1.51968,  0.60375,  1.04918,  0.86946, -0.36072,  0.05575,\n",
       "        0.34953, -0.69543, -1.50081, -0.66540, -1.44589,  0.83264,\n",
       "       -1.20286,  1.22721, -0.60024, -0.91302,  1.68104, -0.49251,\n",
       "       -0.93936, -0.93433, -0.57255,  0.91379,  0.19800,  0.13301,\n",
       "       -0.36928,  1.30388, -0.31906, -0.52012,  0.56380,  1.58696,\n",
       "        1.56608, -1.23120,  0.17912, -0.32235,  1.45419, -1.12321,\n",
       "        0.14135,  0.35670, -1.41297,  1.30972, -0.07865, -1.22982,\n",
       "       -0.98390, -1.42463, -0.73702, -0.07673, -0.39258,  0.84637,\n",
       "        1.42621,  0.68790,  0.93798, -0.68772, -1.05580, -0.85586,\n",
       "        1.45594, -1.35347,  1.17059, -0.32316, -0.14073,  0.79051,\n",
       "        0.24109,  1.02678,  0.56539, -1.41835,  1.48429,  0.54693,\n",
       "       -0.22418,  1.34236, -0.95697, -0.60443,  1.26586,  0.66590,\n",
       "        0.80971, -1.25394,  0.37415, -0.52214, -1.36223,  0.12407,\n",
       "       -1.27544, -1.38427,  0.62874,  1.48573,  0.07224, -1.32783,\n",
       "       -0.54523,  0.55211, -1.24001,  1.59800,  0.19886,  1.49057,\n",
       "       -0.32958, -0.80150, -0.71494, -0.32833,  1.05593, -0.95668])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repetindo o treinamento\n",
    "lr = LinearRegressionGD(n_iter= 500)\n",
    "lr.fit(X_train_std, y_train_std)\n",
    "\n",
    "\n",
    "# Transformando os dados de teste usando a mesma escala utilizada para os dados de treinamento\n",
    "X_test_std = sc_x.transform(X_test)\n",
    "\n",
    "# Aplicando a previsão\n",
    "y_hat_std = lr.predict(X_test_std)\n",
    "\n",
    "y_hat_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que os dados que obtivemos não estão na escala de preços de apartamentos. Precisamos transformá-los.\n",
    "\n",
    "Para isso, usamos a função inversa da transformação de y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sc_y.inverse_transform(y_hat_std.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 413617.61550]\n",
      " [ 792275.23105]\n",
      " [ 529667.93486]\n",
      " [ 456646.85773]\n",
      " [ 974014.95155]\n",
      " [ 972129.60044]\n",
      " [ 495235.89810]\n",
      " [ 832266.35284]\n",
      " [ 144339.41363]\n",
      " [ 585434.33465]\n",
      " [ 794890.64306]\n",
      " [ 745033.90690]\n",
      " [ 599485.61716]\n",
      " [ 262459.19829]\n",
      " [ 225289.84973]\n",
      " [ 175111.76429]\n",
      " [ 851185.75629]\n",
      " [ 172775.78447]\n",
      " [ 323070.19756]\n",
      " [ 308020.55283]\n",
      " [ 777507.98792]\n",
      " [ 719868.26964]\n",
      " [ 89319.60101]\n",
      " [ 299318.95695]\n",
      " [ 590296.57194]\n",
      " [ 749641.53538]\n",
      " [ 604103.07334]\n",
      " [ 661197.84120]\n",
      " [ 297615.25496]\n",
      " [ 942437.27322]\n",
      " [ 625448.84278]\n",
      " [ 934424.09294]\n",
      " [ 235204.16504]\n",
      " [ 1003967.83009]\n",
      " [ 154612.01346]\n",
      " [ 904661.49812]\n",
      " [ 379300.55318]\n",
      " [ 645343.33524]\n",
      " [ 93776.70639]\n",
      " [ 353686.00614]\n",
      " [ 478631.42925]\n",
      " [ 564798.84948]\n",
      " [ 120738.71831]\n",
      " [ 115853.71506]\n",
      " [ 135559.86669]\n",
      " [ 680184.02474]\n",
      " [ 498912.79321]\n",
      " [ 314548.90912]\n",
      " [ 114849.48551]\n",
      " [ 387962.66147]\n",
      " [ 541300.00653]\n",
      " [ 322208.93550]\n",
      " [ 963426.71298]\n",
      " [ 516144.67324]\n",
      " [ 424241.51621]\n",
      " [ 626636.23477]\n",
      " [ 367172.13656]\n",
      " [ 86403.92424]\n",
      " [ 567082.81180]\n",
      " [ 792673.20366]\n",
      " [ 753647.89408]\n",
      " [ 612479.10503]\n",
      " [ 286943.31306]\n",
      " [ 218971.91007]\n",
      " [ 155409.90227]\n",
      " [ 489631.13563]\n",
      " [ 620428.05568]\n",
      " [ 181247.03586]\n",
      " [ 736200.87486]\n",
      " [ 664418.26617]\n",
      " [ 807976.18779]\n",
      " [ 305600.83474]\n",
      " [ 252887.37781]\n",
      " [ 316051.99731]\n",
      " [ 834277.25543]\n",
      " [ 566016.99917]\n",
      " [ 627016.57059]\n",
      " [ 483942.59284]\n",
      " [ 177064.77018]\n",
      " [ 332058.65761]\n",
      " [ 836091.42968]\n",
      " [ 263719.43298]\n",
      " [ 511764.36914]\n",
      " [ 839980.03500]\n",
      " [ 206473.11113]\n",
      " [ 568046.62271]\n",
      " [ 956300.16462]\n",
      " [ 402601.85986]\n",
      " [ 413349.11081]\n",
      " [ 603934.91428]\n",
      " [ 668621.02105]\n",
      " [ 589657.67111]\n",
      " [ 918748.19690]\n",
      " [ 617608.29434]\n",
      " [ 395857.91109]\n",
      " [ 170197.19597]\n",
      " [ 78625.03784]\n",
      " [ 856432.69567]\n",
      " [ 452189.16771]\n",
      " [ 373457.91487]\n",
      " [ 850904.46917]\n",
      " [ 864962.01071]\n",
      " [ 114393.67755]\n",
      " [ 976264.22447]\n",
      " [ 521696.87585]\n",
      " [ 209957.05873]\n",
      " [ 853937.07079]\n",
      " [ 775381.23313]\n",
      " [ 156881.52532]\n",
      " [ 967121.27676]\n",
      " [ 133572.43987]\n",
      " [ 418300.36915]\n",
      " [ 495773.64410]\n",
      " [ 567193.75539]\n",
      " [ 712038.74548]\n",
      " [ 697448.24597]\n",
      " [ 321972.87993]\n",
      " [ 560503.67402]\n",
      " [ 739084.99902]\n",
      " [ 172927.50955]\n",
      " [ 847434.94433]\n",
      " [ 529850.07895]\n",
      " [ 447808.33463]\n",
      " [ 647423.08852]\n",
      " [ 124466.04718]\n",
      " [ 595378.71652]\n",
      " [ 412838.02472]\n",
      " [ 925514.67332]\n",
      " [ 224461.83682]\n",
      " [ 749101.68118]\n",
      " [ 149917.85789]\n",
      " [ 614091.48279]\n",
      " [ 806271.65210]\n",
      " [ 141637.75881]\n",
      " [ 918306.08186]\n",
      " [ 899471.91299]\n",
      " [ 856818.33458]\n",
      " [ 267469.25609]\n",
      " [ 767370.36991]\n",
      " [ 54862.37850]\n",
      " [ 345375.02957]\n",
      " [ 944609.02166]\n",
      " [ 275619.28496]\n",
      " [ 424301.11816]\n",
      " [ 424458.09169]\n",
      " [ 590517.24784]\n",
      " [ 981748.26926]\n",
      " [ 79049.28921]\n",
      " [ 403141.25636]\n",
      " [ 678267.25847]\n",
      " [ 409373.07429]\n",
      " [ 306341.07762]\n",
      " [ 164899.58315]\n",
      " [ 175940.03850]\n",
      " [ 567133.15836]\n",
      " [ 750263.97102]\n",
      " [ 359376.76077]\n",
      " [ 663635.81620]\n",
      " [ 439748.51868]\n",
      " [ 841803.97120]\n",
      " [ 507895.31629]\n",
      " [ 845047.42717]\n",
      " [ 536238.64020]\n",
      " [ 215685.58686]\n",
      " [ 575955.73433]\n",
      " [ 252021.91830]\n",
      " [ 637063.53010]\n",
      " [ 289807.86947]\n",
      " [ 886887.96381]\n",
      " [ 484817.84211]\n",
      " [ 722438.96919]\n",
      " [ 506349.18582]\n",
      " [ 325413.87649]\n",
      " [ 813187.84277]\n",
      " [ 138124.86890]\n",
      " [ 618229.01606]\n",
      " [ 272056.06213]\n",
      " [ 177055.38758]\n",
      " [ 628023.66577]\n",
      " [ 494778.76961]\n",
      " [ 484188.38111]\n",
      " [ 275120.92989]\n",
      " [ 118805.55740]\n",
      " [ 488741.17253]\n",
      " [ 448381.16129]\n",
      " [ 761661.12569]\n",
      " [ 679635.91530]\n",
      " [ 426876.33863]\n",
      " [ 676060.72749]\n",
      " [ 697061.05219]\n",
      " [ 831568.97500]\n",
      " [ 471869.12156]\n",
      " [ 360142.10471]\n",
      " [ 829573.02260]\n",
      " [ 326616.49164]\n",
      " [ 955832.97646]\n",
      " [ 94073.19909]\n",
      " [ 133927.36917]\n",
      " [ 149466.95401]\n",
      " [ 453736.90589]\n",
      " [ 137035.35282]\n",
      " [ 943140.88497]\n",
      " [ 602994.30388]\n",
      " [ 92648.12279]\n",
      " [ 944127.99504]\n",
      " [ 697305.92196]\n",
      " [ 817339.36498]\n",
      " [ 768910.33909]\n",
      " [ 437404.36101]\n",
      " [ 549633.24444]\n",
      " [ 628799.86984]\n",
      " [ 347209.79154]\n",
      " [ 130179.41630]\n",
      " [ 355302.07438]\n",
      " [ 144978.90300]\n",
      " [ 758987.65105]\n",
      " [ 210468.22251]\n",
      " [ 865313.24875]\n",
      " [ 372861.69580]\n",
      " [ 288573.91165]\n",
      " [ 987611.34553]\n",
      " [ 401892.36112]\n",
      " [ 281476.43330]\n",
      " [ 282832.19557]\n",
      " [ 380323.49532]\n",
      " [ 780855.98600]\n",
      " [ 587967.45673]\n",
      " [ 570453.93622]\n",
      " [ 435100.15331]\n",
      " [ 885976.03976]\n",
      " [ 448630.96143]\n",
      " [ 394450.20971]\n",
      " [ 686542.37736]\n",
      " [ 962257.70803]\n",
      " [ 956632.53877]\n",
      " [ 202832.22764]\n",
      " [ 582879.17669]\n",
      " [ 447744.08912]\n",
      " [ 926479.17312]\n",
      " [ 231932.35942]\n",
      " [ 572701.20319]\n",
      " [ 630733.13925]\n",
      " [ 153849.61283]\n",
      " [ 887549.08914]\n",
      " [ 513416.03997]\n",
      " [ 203204.11851]\n",
      " [ 269473.58634]\n",
      " [ 150708.18878]\n",
      " [ 336000.84162]\n",
      " [ 513935.19225]\n",
      " [ 428820.09820]\n",
      " [ 762687.02787]\n",
      " [ 918940.73965]\n",
      " [ 719984.21663]\n",
      " [ 787372.79206]\n",
      " [ 349287.08787]\n",
      " [ 250097.67834]\n",
      " [ 303976.29522]\n",
      " [ 926951.40054]\n",
      " [ 169883.72383]\n",
      " [ 850056.13724]\n",
      " [ 447528.31457]\n",
      " [ 496688.19223]\n",
      " [ 747632.81969]\n",
      " [ 599577.83230]\n",
      " [ 811303.27219]\n",
      " [ 686969.21325]\n",
      " [ 152399.37267]\n",
      " [ 934591.21796]\n",
      " [ 681996.09975]\n",
      " [ 474200.15873]\n",
      " [ 896342.87581]\n",
      " [ 276730.80587]\n",
      " [ 371731.53313]\n",
      " [ 875728.80433]\n",
      " [ 714054.56560]\n",
      " [ 752808.19814]\n",
      " [ 196703.29281]\n",
      " [ 635434.98038]\n",
      " [ 393906.37814]\n",
      " [ 167523.63094]\n",
      " [ 568043.99034]\n",
      " [ 190912.11500]\n",
      " [ 161582.46865]\n",
      " [ 704041.14588]\n",
      " [ 934978.09817]\n",
      " [ 554077.58879]\n",
      " [ 176793.97218]\n",
      " [ 387684.15942]\n",
      " [ 683391.93246]\n",
      " [ 200458.25778]\n",
      " [ 965233.42617]\n",
      " [ 588199.56142]\n",
      " [ 936283.71183]\n",
      " [ 445796.93812]\n",
      " [ 318625.32776]\n",
      " [ 341951.08400]\n",
      " [ 446135.16243]\n",
      " [ 819157.10825]\n",
      " [ 276810.18210]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.5f}'.format})\n",
    "\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 374670.69148,  800391.62942,  493671.34772,  429569.52006,\n",
       "        1032790.11930,  1044645.76883,  463790.43167,  855601.06787,\n",
       "        182249.02104,  554162.58354,  831492.25451,  732268.25367,\n",
       "        569347.36670,  257091.75814,  260736.50502,  212124.66539,\n",
       "        889709.76450,  215741.12036,  287187.71980,  306709.20766,\n",
       "        757319.26574,  706097.67893,  143006.26140,  286492.06447,\n",
       "        556173.77255,  742616.12088,  592120.30084,  630047.35682,\n",
       "        317681.53730,  997203.13343,  596255.66247,  983394.65964,\n",
       "        219255.70104,  1076203.73944,  175767.39781,  931463.42497,\n",
       "        346378.01132,  620102.65798,  152424.31784,  316189.39860,\n",
       "        455312.04457,  524790.61116,  178517.97659,  153797.32280,\n",
       "        180072.66968,  652937.45459,  475941.44174,  303843.33742,\n",
       "        161425.80368,  364619.96035,  494444.37968,  323373.06857,\n",
       "        1032362.39393,  474454.29524,  393300.54893,  585825.79154,\n",
       "        342831.25804,  152316.19253,  521461.03163,  791248.47971,\n",
       "        758086.54963,  577213.64302,  288110.29104,  237578.19621,\n",
       "        182691.74860,  449261.21159,  599195.86846,  201969.16058,\n",
       "        743332.81029,  641232.70844,  806944.34870,  299517.14593,\n",
       "        262440.61406,  299202.86735,  836956.49604,  540629.10564,\n",
       "        590213.59247,  440603.42627,  197746.65010,  303938.07850,\n",
       "        876965.24786,  286615.29116,  465104.61242,  829133.25384,\n",
       "        197255.46903,  553062.38912,  998470.59867,  377113.51160,\n",
       "        389834.77408,  556957.54151,  648327.81775,  567778.25777,\n",
       "        995554.05461,  612081.68727,  360257.80418,  214832.68997,\n",
       "        141774.75362,  891212.64625,  435451.19954,  334177.25597,\n",
       "        867132.38486,  880502.69558,  172976.14543,  1036256.72790,\n",
       "        476314.68207,  225971.00881,  887955.98429,  753380.02578,\n",
       "        210223.47323,  1018201.25481,  171139.61651,  377015.62515,\n",
       "        470517.17322,  552171.49765,  696897.59248,  666815.01116,\n",
       "        295278.37727,  530359.45591,  726002.37018,  198466.22786,\n",
       "        865584.76452,  488300.50213,  419301.96850,  621633.65697,\n",
       "        172543.06781,  561952.84853,  397701.26138,  1001896.53856,\n",
       "        231444.05427,  742210.29735,  194636.98021,  568329.07216,\n",
       "        800854.32138,  170232.82323,  942227.63204,  949915.10006,\n",
       "        846571.86969,  264489.58357,  762554.27007,  108748.44649,\n",
       "        340937.80363,  1004843.50363,  268627.65794,  392881.97240,\n",
       "        403135.98362,  561934.00249,  1080227.16899,  146312.15585,\n",
       "        369843.12611,  659016.94291,  388157.63399,  274467.89578,\n",
       "        233465.86328,  216045.23042,  524642.92281,  736873.54736,\n",
       "        348564.58925,  639669.31838,  400416.49316,  834587.72114,\n",
       "        478583.91996,  860570.45001,  478001.25522,  254672.15457,\n",
       "        517116.07197,  258565.30971,  609891.56483,  271818.84864,\n",
       "        906036.32026,  452017.88086,  679692.01918,  463474.77145,\n",
       "        306633.54208,  829919.05705,  193108.23025,  591278.16147,\n",
       "        262135.29637,  213342.03855,  601613.29246,  468629.56355,\n",
       "        448056.05306,  292875.45130,  177847.70339,  473115.17784,\n",
       "        416014.24230,  754348.29485,  643862.97376,  382563.59348,\n",
       "        630181.06478,  661116.16939,  870114.91862,  429570.90092,\n",
       "        337519.09859,  842089.63333,  326558.21572,  990334.06458,\n",
       "        139468.84752,  189572.91348,  191797.58237,  410370.04554,\n",
       "        180524.73724,  995780.03987,  570455.49330,  163250.87557,\n",
       "        968825.63190,  677585.96154,  838208.54998,  769828.36899,\n",
       "        435594.34666,  507158.51461,  590915.92785,  321653.47781,\n",
       "        198555.03667,  329483.15929,  162966.44231,  750272.26910,\n",
       "        228873.21818,  880327.70840,  354398.66314,  295440.32853,\n",
       "        1053952.94464,  378456.10562,  258803.07295,  282709.67736,\n",
       "        357763.93564,  772991.57144,  562648.28525,  525841.14480,\n",
       "        409589.59312,  889680.74310,  411933.85323,  375522.60562,\n",
       "        682305.11563,  1025225.90398,  1017977.32683,  220343.60741,\n",
       "        539242.39362,  439132.54135,  991275.95485,  260351.43049,\n",
       "        539100.67306,  592151.19159,  198305.50828,  945241.17282,\n",
       "        460214.42496,  236354.47306,  279867.89404,  201592.49183,\n",
       "        306645.54840,  476423.50597,  397764.25434,  710862.85082,\n",
       "        983624.33701,  691573.46065,  762399.37993,  327181.80204,\n",
       "        268209.00897,  290587.11237,  952808.05919,  172504.97460,\n",
       "        856502.80605,  407527.61874,  456309.18959,  724265.23181,\n",
       "        563363.16914,  846421.78367,  670079.81726,  164498.90767,\n",
       "        978498.83513,  675861.65513,  437931.71934,  925761.92696,\n",
       "        273152.01347,  357140.18848,  918889.36684,  728632.18993,\n",
       "        749941.47900,  224885.07270,  609131.11480,  359822.19940,\n",
       "        189314.99618,  515124.21576,  209311.99148,  227464.13834,\n",
       "        669209.38451,  1004552.08241,  514920.41140,  219550.97489,\n",
       "        355939.92903,  658089.04400,  205775.06273,  1015341.51568,\n",
       "        556924.36304,  996649.23521,  419967.54164,  299404.47179,\n",
       "        318349.58211,  422516.64065,  853776.88695,  263133.80721])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.98246  0.10890 -0.03260]\n",
      "b: [-0.00000]\n",
      "MSE: 1252460721.4512322\n",
      "MAE: 30499.403439670397\n"
     ]
    }
   ],
   "source": [
    "print(f\"w: {lr.w_}\")\n",
    "print(f\"b: {lr.b_}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_hat)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando a biblioteca de Regressão Linear do Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.98914  0.11038 -0.03381]\n",
      "b: -3.0520616263739904e-15\n",
      "MSE: 1254044673.0267076\n",
      "MAE: 30545.752993863447\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_std, y_train_std)\n",
    "y_pred_std = lin_reg.predict(X_test_std)\n",
    "\n",
    "y_pred = sc_y.inverse_transform(y_pred_std.reshape(-1, 1))\n",
    "\n",
    "print(f\"w: {lin_reg.coef_}\")\n",
    "print(f\"b: {lin_reg.intercept_}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Transformações Polinomiais\n",
    "\n",
    "\n",
    "Apesar de chamado de Regressão Linear, alguns ajustes nos dados podem deixar o modelo mais complexo, adicionando a possibilidade de não linearidade durante o treinamento. \n",
    "\n",
    "Por exemplo, a função:\n",
    "\n",
    "- $f(x) = w_1 x_1 + w_2 x_2 + b $\n",
    "\n",
    "\n",
    "Poderia ser redefinida como:\n",
    "\n",
    "- $f(x) = w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 + w_5 x_1 x_2 + b $\n",
    "\n",
    "\n",
    "Perceba que o modelo continua linear do ponto de vista dos coeficientes. A não linearidade está nos atributos!\n",
    "\n",
    "Em outras palavras, estamos criando \"novos atributos\" a partir dos atributos originais do conjunto de dados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [ 0.98577  0.11161 -0.02613  0.14042  0.00202  0.00667  0.00392 -0.00113\n",
      "  0.00052]\n",
      "b: -0.14510391779858156\n",
      "MSE: 97334573.06357545\n",
      "MAE: 8085.0771911721895\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_poly_train_std = poly_features.fit_transform(X_train_std)\n",
    "X_poly_test_std = poly_features.transform(X_test_std)\n",
    "\n",
    "lin_reg.fit(X_poly_train_std, y_train_std)\n",
    "y_poly_pred_std = lin_reg.predict(X_poly_test_std)\n",
    "\n",
    "y_poly_pred = sc_y.inverse_transform(y_poly_pred_std.reshape(-1, 1))\n",
    "\n",
    "print(f\"w: {lin_reg.coef_}\")\n",
    "print(f\"b: {lin_reg.intercept_}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_poly_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_poly_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando modelos mais robustos\n",
    "\n",
    "\n",
    "Algumas variações do modelo de Regressão Linar incorporam penalidades ao uso dos coeficientes.\n",
    "\n",
    "A ideia é evitar um superajuste aos dados de treinamento e melhorar, assim, a generalização do modelo.\n",
    "\n",
    "Em situações mais complexas esse efeito é importante. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = X_poly_train_std\n",
    "X_test_std = X_poly_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 107424131.21913005\n",
      "MAE: 8405.33772042533\n"
     ]
    }
   ],
   "source": [
    "#Ridge Regression\n",
    "\n",
    "ridge_reg = Ridge(alpha=1)\n",
    "ridge_reg.fit(X_train_std, y_train_std)\n",
    "y_ridge_pred_std = ridge_reg.predict(X_test_std)\n",
    "y_ridge_pred = sc_y.inverse_transform(y_ridge_pred_std.reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_ridge_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_ridge_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2390250237.566013\n",
      "MAE: 41904.16944608518\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X_train_std, y_train_std)\n",
    "y_lasso_pred_std = lasso_reg.predict(X_test_std)\n",
    "y_lasso_pred = sc_y.inverse_transform(y_lasso_pred_std.reshape(-1, 1))\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_lasso_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_lasso_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1958937761.6558297\n",
      "MAE: 37330.54730440226\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net Regression\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train_std, y_train_std)\n",
    "y_elastic_pred_std = elastic_net.predict(X_test_std)\n",
    "y_elastic_pred = sc_y.inverse_transform(y_elastic_pred_std.reshape(-1, 1))\n",
    "\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_elastic_pred)}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_elastic_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
