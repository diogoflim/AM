{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/diogoflim/AM/blob/main/3_GradienteDescendente_RL/gdlr.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado de máquina e decisões dirigidas por dados\n",
    "\n",
    "**Professor: Diogo Ferreira de Lima Silva**\n",
    "\n",
    "**TPP - UFF**\n",
    "\n",
    "**Aula 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente Descendente em Regressão Linear\n",
    "\n",
    "Nessa aula, vamos implementar o algoritmo gradiente descendente para regressão linear.\n",
    "\n",
    "Inicialmente, trabalharemos o caso com 1 atributo e depois passaremos para o caso genérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bibliotecas\n",
    "import copy\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso com 1 Atributo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciaremos criando um vetor de valores aleatórios para o nosso alearórios e os respectivos rótulos seguindo um relacionamento linear, conforme realizado na aula 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando valores para o nosso atributo x\n",
    "mu, sigma = 150, 50 # média e desvio padrão\n",
    "X = np.random.normal(mu, sigma, size=(100,1)) \n",
    "\n",
    "# Gerando os rótulos\n",
    "mu_2, sigma_2 = 0, 100\n",
    "y = 25 + 2 * X + np.random.normal(mu_2, sigma_2, size=(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os 10 primeiros valores de x: \n",
      "[[174.03452572]\n",
      " [118.35302752]\n",
      " [196.41698734]\n",
      " [ 79.09681178]\n",
      " [230.01085011]\n",
      " [ 43.31802831]\n",
      " [180.69069758]\n",
      " [240.10315475]\n",
      " [164.12039992]\n",
      " [183.16377668]]\n",
      "Os 10 primeiros valores de y: \n",
      "[[490.78778512]\n",
      " [281.27756992]\n",
      " [288.96417806]\n",
      " [183.27561887]\n",
      " [683.49178105]\n",
      " [270.41375252]\n",
      " [380.94500312]\n",
      " [436.05420878]\n",
      " [329.69091821]\n",
      " [331.47930387]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Os 10 primeiros valores de x: \\n{X[:10]}\")\n",
    "print(f\"Os 10 primeiros valores de y: \\n{y[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custo_total (x, y, w, b):\n",
    "    m = x.shape[0] \n",
    "    custo = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        custo = custo + (f_wb - y[i])**2 \n",
    "    J = (1 / (2 * m)) * custo\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67467.39176398])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custo_total(X,y, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(x, y, w, b): \n",
    "    m = x.shape[0] # numero de exemplos   \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    # loop nos exemplos \n",
    "    for i in range(m):  \n",
    "        # Primeiro calculamos o valor da função linear no exemplo i\n",
    "        f_wb = w * x[i] + b \n",
    "        # Calculamos a contribuição do exemplo i para as derivadas (no somatório)\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        # Acrescentamos a contribuição de i no valor das derivadas\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    # Ao deixar o loop, temos o valor dos somatórios associados às derivadas.\n",
    "    # Ainda precisamos dividir por m\n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_descente (x, y, w_in, b_in, alfa, num_iters, f_custo, f_gradiente): \n",
    "    # Por precaução, vamos guardar o valor de nossos chutes iniciais \n",
    "    w = copy.deepcopy(w_in) \n",
    "    J_lista = []\n",
    "    parametros_lista = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    # Em cada iteração i\n",
    "    for i in range(num_iters):\n",
    "        # Calculamos o valor das derivadas parciais com a função gradiente\n",
    "        dj_dw, dj_db = f_gradiente(x, y, w ,b)     \n",
    "        # Atualizamos o valor dos parâmetros w e b\n",
    "        b = b - alfa * dj_db                            \n",
    "        w = w - alfa * dj_dw                            \n",
    "        # Salvamos o valor de J no iteração i\n",
    "        J_lista.append(f_custo(x, y, w, b))\n",
    "        parametros_lista.append([w,b])\n",
    "    \n",
    "    return w, b, J_lista, parametros_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos normalizar os valores de X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_norm = MinMaxScaler().fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58355198],\n",
       "       [0.33497549],\n",
       "       [0.68347303],\n",
       "       [0.15972567],\n",
       "       [0.83344465],\n",
       "       [0.        ],\n",
       "       [0.61326684],\n",
       "       [0.87849929],\n",
       "       [0.53929278],\n",
       "       [0.6243073 ]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[490.78778512],\n",
       "       [281.27756992],\n",
       "       [288.96417806],\n",
       "       [183.27561887],\n",
       "       [683.49178105],\n",
       "       [270.41375252],\n",
       "       [380.94500312],\n",
       "       [436.05420878],\n",
       "       [329.69091821],\n",
       "       [331.47930387]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, J_lista, parametros_lista = gradiente_descente (X, y, w_in = 0.0, b_in = 0.0, \n",
    "                                                     alfa = 0.00003, num_iters = 1000, \n",
    "                                                    f_custo = custo_total, f_gradiente = gradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.10798306]), array([8.56651248e-05]))"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5135.75705664])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custo Final \n",
    "J_lista[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos lembrar dos nossos valores ótimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.73655292],\n",
       "       [ 1.94092938]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_novo = np.concatenate([np.ones(shape=(X.shape)), X], axis=1)\n",
    "\n",
    "w_hat = np.linalg.inv(X_novo.T.dot(X_novo)).dot(X_novo.T).dot(y)\n",
    "\n",
    "w_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
