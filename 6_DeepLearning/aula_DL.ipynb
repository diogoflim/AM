{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312233b5",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/diogoflim/AM/blob/main/6_DeepLearning/aula_DL.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21e6e8-ca56-42bd-9f00-6409f8a17dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aprendizado de máquina e decisões dirigidas por dados\n",
    "\n",
    "**Professor: Diogo Ferreira de Lima Silva**\n",
    "\n",
    "**TPP - UFF**\n",
    "\n",
    "**Aula 6 e 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisão das últimas aulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudamos nas últimas aulas o modelo Perceptron, conhecido como o modelo básico de redes neurais. Vimos que\n",
    "\n",
    "- O Perceptron, assim como uma regressão linear ou uma regressão logística, pode ser visto como uma rede de: \n",
    "\n",
    "    - uma camada de input, representada pelos atributos das observações de treinamento.\n",
    "    - uma camada escondida com um único neurônio que recebe a soma ponderada dos valores dos atributos pelos respectivos pessos.\n",
    "        - observação: também há um atributo conhecido como bias $(b)$.\n",
    "    - Na camada escondida, realiza-se uma operação designada pela função de ativação a depender do modelo utilizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura abaixo ilustra a situação:\n",
    "\n",
    "<img src=\"fig_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de ativação para os casos estudados seriam:\n",
    "\n",
    "- Regressão Linear: $a = g(z)=z$\n",
    "\n",
    "- Regressão Logística: $a = g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "    - Nesse caso: \n",
    "    \n",
    "    $$y = \\begin{cases} 1 & \\text{se } a > 0.5 \\\\ 0 & \\text{caso contrário} \\end{cases}$$\n",
    "\n",
    "- Perceptron: $a = g(z)=z$\n",
    "    - Nesse caso: \n",
    "    \n",
    "    $$y = \\begin{cases} 1 & \\text{se } a >= 0 \\\\ -1 & \\text{caso contrário} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Múltiplas Camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, o estudo de redes neurais trata de problemas onde é interessante a utilização de múltiplas camadas escondidas. Nesse caso, nosso algoritmo de aprendizagem ganha a capacidade de aprender funções mais complexas. \n",
    "\n",
    "Adicionamente, várias outras possibilidades de funções de ativação podem ser utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura de uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma RNA, os neurônios podem estar dispostos em mais de uma camada intermediária. \n",
    "\n",
    "- Saídas dos neurônios de uma camada intermediária podem ser entradas para os neurônios da camada intermediária seguinte. \n",
    "- Saídas dos neurônios da última camada intermediária são entradas para neurônios dispostos em uma camada de saída (output). \n",
    "\n",
    "Quando há mais de uma camada intermediária, a rede neural é chamada de rede multicamadas (redes profundas). \n",
    "\n",
    "Vejamos um exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig_2.png\">\n",
    "\n",
    "Nessa figura, apenas alguns arcos foram desenhados para simplificar a visualização. No entanto, perceba que a partir de um neurônio na camada $l$ poderia sair um link para cada neurônio da camada $l+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos uma representação simplificada da rede acima, porém, considere que temos todos os links propagados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguinte notação será utilizada para uma rede neural multicamada:\n",
    "\n",
    "- O número de camadas na rede é dado por $L$ e o número de neurônios em uma camada $l$ é dado por $n^[l]$ , sendo $n^[0] =n$ o número de atributos.\n",
    "- Um exemplo de treinamento é representado por um vetor coluna de $n$  atributos.\n",
    "\n",
    "$$ \\vec{x}^{(i)} \\in \\mathbb{R}^{n^{[0]}} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]^T $$\n",
    "\n",
    "- A matriz $ \\mathbf{X} \\in \\mathbb{R}^{n^{[0]} \\times m}$ organiza cada um dos $m$ exemplos de treinamento em uma coluna, assim, $\\mathbf{X}_{:,i} = x^{(i)}$.\n",
    "\n",
    "- $y^{(i)}$ representa um rótulo atribuído ao exemplo $\\vec{x}^{(i)}$. O vetor $\\vec{y} = [y^{(i)},…,y^{(m)}] $ representa os rótulos do conjunto de treinamento. \n",
    "\n",
    "- Uma matriz $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{n^[l-1] \\times n^[l]}$ representa os pesos utilizados para ponderar a entrada de uma camada $l$ com uma equação linear do tipo: $wx+b$. A entrada da matriz na linha $i$ e coluna $j$, $W_{i,j}^{[l]}$ , representa o peso associado a uma entrada no neurônio $j$  da camada $l$ que corresponde a uma saída do neurônio $i$ da camada $(l-1)$. \n",
    "\n",
    "- O vetor coluna $\\vec{b}^{[l]}$  recebe os valores dos interceptos das $n^{[l]}$  equações lineares correspondentes às entradas dos neurônios da camada $l$.\n",
    "\n",
    "- $\\vec{z}^{[l]} = \\mathbf{W}^{[l]^T} \\cdot \\vec{a}^{[l-1]} + \\vec{b}^{[l]}$, onde $\\vec{z}^{[l]} \\in \\mathbb{R}^{n^[l]}$, representa um vetor coluna com a ponderação das entradas nos neurônios da camada $l$. Na primeira camada intermediária, o exemplo de treinamento é ponderado, $a^[0] =x^{(i)}$.\n",
    "\n",
    "- Uma matriz $\\mathbf{Z}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m} $ representa as ponderações das entradas para os $m$ diferentes exemplos do conjunto de treinamento Dessa forma: $\\mathbf{A}^{[0]} = \\mathbf{X}$. \n",
    "\n",
    "- $\\vec{a}^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ representa a saída da função de ativação $g^[l]$ aplicada na camada $l$. Assim, para uma camada $l$ temos: $\\vec{a}^{[l]}=g^{[l]} (\\vec{z}^{[l]})$.\n",
    "\n",
    "- Uma matriz $\\mathbf{A}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ representa as saídas das funções de ativação em cada camada $l$ (linhas) para os $m$ exemplos de treinamento (colunas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
