{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312233b5",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/diogoflim/AM/blob/main/6_DeepLearning/aula_DL.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21e6e8-ca56-42bd-9f00-6409f8a17dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Aprendizado de máquina e decisões dirigidas por dados\n",
    "\n",
    "**Professor: Diogo Ferreira de Lima Silva**\n",
    "\n",
    "**TPP - UFF**\n",
    "\n",
    "**Aula 6 e 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisão das últimas aulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudamos nas últimas aulas o modelo Perceptron, conhecido como o modelo básico de redes neurais. Vimos que\n",
    "\n",
    "- O Perceptron, assim como uma regressão linear ou uma regressão logística, pode ser visto como uma rede de: \n",
    "\n",
    "    - uma camada de input, representada pelos atributos das observações de treinamento.\n",
    "    - uma camada escondida com um único neurônio que recebe a soma ponderada dos valores dos atributos pelos respectivos pessos.\n",
    "        - observação: também há um atributo conhecido como bias $(b)$.\n",
    "    - Na camada escondida, realiza-se uma operação designada pela função de ativação a depender do modelo utilizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura abaixo ilustra a situação:\n",
    "\n",
    "<img src=\"fig_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de ativação para os casos estudados seriam:\n",
    "\n",
    "- Regressão Linear: $a = g(z)=z$\n",
    "\n",
    "- Regressão Logística: $a = g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "    - Nesse caso: \n",
    "    \n",
    "    $$y = \\begin{cases} 1 & \\text{se } a > 0.5 \\\\ 0 & \\text{caso contrário} \\end{cases}$$\n",
    "\n",
    "- Perceptron: $a = g(z)=z$\n",
    "    - Nesse caso: \n",
    "    \n",
    "    $$y = \\begin{cases} 1 & \\text{se } a >= 0 \\\\ -1 & \\text{caso contrário} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Múltiplas Camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, o estudo de redes neurais trata de problemas onde é interessante a utilização de múltiplas camadas escondidas. Nesse caso, nosso algoritmo de aprendizagem ganha a capacidade de aprender funções mais complexas. \n",
    "\n",
    "Adicionamente, várias outras possibilidades de funções de ativação podem ser utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura de uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma RNA, os neurônios podem estar dispostos em mais de uma camada intermediária. \n",
    "\n",
    "- Saídas dos neurônios de uma camada intermediária podem ser entradas para os neurônios da camada intermediária seguinte. \n",
    "- Saídas dos neurônios da última camada intermediária são entradas para neurônios dispostos em uma camada de saída (output). \n",
    "\n",
    "Quando há mais de uma camada intermediária, a rede neural é chamada de rede multicamadas (redes profundas). \n",
    "\n",
    "Vejamos um exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig_2.png\">\n",
    "\n",
    "Nessa figura, apenas alguns arcos foram desenhados para simplificar a visualização. No entanto, perceba que a partir de um neurônio na camada $l$ poderia sair um link para cada neurônio da camada $l+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos uma representação simplificada da rede acima, porém, considere que temos todos os links propagados.\n",
    "\n",
    "<img src = \"fig_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetor $\\vec{a}^{[0]} = \\vec{x} = [x_1, x_2, ..., x_n]$.\n",
    "\n",
    "---\n",
    "\n",
    "Vetor $\\vec{a}^{[1]}$:\n",
    "\n",
    "$$ a^{[1]}_1 =  g^{[1]}(\\vec{w}^{[1]}_1 \\cdot \\vec{x} + b^{[1]}_1) $$\n",
    "$$ a^{[1]}_2 =  g^{[1]}(\\vec{w}^{[1]}_2 \\cdot \\vec{x} + b^{[1]}_2) $$\n",
    "$$ a^{[1]}_3 =  g^{[1]}(\\vec{w}^{[1]}_3 \\cdot \\vec{x} + b^{[1]}_3) $$\n",
    "\n",
    "--------\n",
    "\n",
    "Vetor $\\vec{a}^{[2]}$:\n",
    "\n",
    "$$ a^{[2]}_1 =  g^{[2]}(\\vec{w}^{[2]}_1 \\cdot \\vec{a}^{[1]} + b^{[2]}_1) $$\n",
    "$$ a^{[2]}_2 =  g^{[2]}(\\vec{w}^{[2]}_2 \\cdot \\vec{a}^{[1]} + b^{[2]}_2) $$\n",
    "$$ a^{[2]}_3 =  g^{[2]}(\\vec{w}^{[2]}_3 \\cdot \\vec{a}^{[1]} + b^{[2]}_3) $$\n",
    "$$ a^{[2]}_4 =  g^{[2]}(\\vec{w}^{[2]}_4 \\cdot \\vec{a}^{[1]} + b^{[2]}_4) $$\n",
    "\n",
    "--------\n",
    "\n",
    "Vetor $\\vec{a}^{[3]}$:\n",
    "\n",
    "$$ a^{[3]}_1 =  g^{[3]}(\\vec{w}^{[3]}_1 \\cdot \\vec{a}^{[2]} + b^{[3]}_1) $$\n",
    "$$ a^{[3]}_2 =  g^{[3]}(\\vec{w}^{[3]}_2 \\cdot \\vec{a}^{[2]} + b^{[3]}_2) $$\n",
    "\n",
    "--------\n",
    "\n",
    "De maneira geral, a notação fica:\n",
    "\n",
    "$$ a^{[l]}_j =  g^{[l]}(\\vec{w}^{[l]}_j \\cdot \\vec{a}^{[l-1]} + b^{[l]}_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que, na notação que estudamos até o momento, estamos tratando da passagem de um exemplo (um vetor $\\vec{x}$ específico) pela rede. \n",
    "\n",
    "Porém, na verdade uma matriz de exemplos de treinamento $\\mathbf{X}$ deve passar pela rede para o cálculo do custo após cada iteração.\n",
    "\n",
    "**Vamos iniciar um primeiro exemplo no Tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "O TensorFlow é uma das principais bibliotecas para implementação de algorítmos de redes neurais. \n",
    "\n",
    "Vejamos como ficaria o exemplo de uma rede neural de 3 camadas escondidas implementado no TensorFlow:\n",
    "\n",
    "- Camada 1 com 3 neurônios\n",
    "- Camada 2 com 4 neurônios\n",
    "- Camada 3 com 1 neurônio\n",
    "- As funções de ativação em todas as camadas são sigmoides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234) \n",
    "\n",
    "modelo = Sequential(\n",
    "    [\n",
    "        Dense(3, activation='sigmoid', name = 'camada_1'),\n",
    "        Dense(4, activation='sigmoid', name = 'camada_2'),\n",
    "        Dense(1, activation='sigmoid', name = 'camada_3')\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um Dataset para testar nosso modelo. \n",
    "O problema tratado será de duas classes. Usaremos a função do sklearn makeblobs para criar o dataset com as seguintes características:\n",
    "- 2 classes\n",
    "- 3 atributos\n",
    "- 1000 exemplos\n",
    "\n",
    "Informações: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "classes = 2\n",
    "m = 1000\n",
    "centers = [[-5, -5, -5], [5, 5, 5]]\n",
    "std = 2.0\n",
    "X_train, y_train = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20688738,  1.85542306,  4.06406343],\n",
       "       [-2.63126111, -5.21169238, -6.3518573 ],\n",
       "       [ 4.52408155,  5.35737499,  5.90396342],\n",
       "       ...,\n",
       "       [ 2.6673297 ,  5.73189276,  4.18384554],\n",
       "       [-3.17979453, -3.51453902, -3.90060731],\n",
       "       [-7.91712827, -3.94257525, -5.05089646]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de exemplos na classe 0: 500\n",
      "Número de exemplos na classe 1: 500\n"
     ]
    }
   ],
   "source": [
    "contagem = np.bincount(y_train)\n",
    "print(f\"Número de exemplos na classe 0: {contagem[0]}\\nNúmero de exemplos na classe 1: {contagem[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passando os exemplos para o treinamento do nosso modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 2s 4ms/step - loss: 0.7406\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6243\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4789\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2901\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.1587\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0956\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0650\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0479\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0373\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0301\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0250\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0212\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0125\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0112\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0101\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0092\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad7d5960a0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 0.08334005 -0.29660565  0.17884266]\n",
      " [-0.56124383 -0.15262699  0.8899205 ]] \n",
      "b1: [0. 0. 0.]\n",
      "W2:\n",
      " [[-0.32336175 -0.66899645  0.27413416 -0.36288744]\n",
      " [-0.3022557  -0.68477273  0.38106263  0.40476227]\n",
      " [-0.20340782  0.56833684 -0.0420264   0.01816851]] \n",
      "b2: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = model.get_layer(\"camada_1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"camada_2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n",
      "As previsões para o valor da probabilidade de x_1 pertencer à classe 1 são: \n",
      "[[0.99113667]\n",
      " [0.00738508]\n",
      " [0.9914465 ]\n",
      " [0.99142814]\n",
      " [0.00738365]\n",
      " [0.9914479 ]\n",
      " [0.99144655]\n",
      " [0.9914486 ]\n",
      " [0.9914481 ]\n",
      " [0.00739208]\n",
      " [0.00738243]\n",
      " [0.99140877]\n",
      " [0.00740165]\n",
      " [0.9914396 ]\n",
      " [0.99144435]\n",
      " [0.00739137]\n",
      " [0.9914484 ]\n",
      " [0.99144095]\n",
      " [0.9913723 ]\n",
      " [0.99144775]\n",
      " [0.0073979 ]\n",
      " [0.9914406 ]\n",
      " [0.00740612]\n",
      " [0.99144423]\n",
      " [0.9914475 ]\n",
      " [0.00738364]\n",
      " [0.00738276]\n",
      " [0.99143916]\n",
      " [0.991448  ]\n",
      " [0.9913822 ]\n",
      " [0.99144876]\n",
      " [0.991439  ]\n",
      " [0.9914414 ]\n",
      " [0.00739415]\n",
      " [0.00738236]\n",
      " [0.00738691]\n",
      " [0.00738426]\n",
      " [0.00738792]\n",
      " [0.00738258]\n",
      " [0.991448  ]\n",
      " [0.00743271]\n",
      " [0.007393  ]\n",
      " [0.00738147]\n",
      " [0.9914462 ]\n",
      " [0.9914468 ]\n",
      " [0.00738444]\n",
      " [0.9914462 ]\n",
      " [0.9914485 ]\n",
      " [0.00738792]\n",
      " [0.9914446 ]\n",
      " [0.99143684]\n",
      " [0.9914437 ]\n",
      " [0.00740264]\n",
      " [0.00738253]\n",
      " [0.99144244]\n",
      " [0.99144614]\n",
      " [0.9914465 ]\n",
      " [0.99144506]\n",
      " [0.00739992]\n",
      " [0.00739963]\n",
      " [0.00738807]\n",
      " [0.99144125]\n",
      " [0.00738659]\n",
      " [0.00739049]\n",
      " [0.99135756]\n",
      " [0.9914249 ]\n",
      " [0.00740982]\n",
      " [0.9914458 ]\n",
      " [0.00739938]\n",
      " [0.00738196]\n",
      " [0.99144715]\n",
      " [0.00738184]\n",
      " [0.00738314]\n",
      " [0.9914403 ]\n",
      " [0.00738151]\n",
      " [0.01232222]\n",
      " [0.00738126]\n",
      " [0.99135846]\n",
      " [0.00779023]\n",
      " [0.99144465]\n",
      " [0.9914405 ]\n",
      " [0.00738126]\n",
      " [0.9914454 ]\n",
      " [0.9914479 ]\n",
      " [0.99144876]\n",
      " [0.9914422 ]\n",
      " [0.00740291]\n",
      " [0.00738314]\n",
      " [0.00738115]\n",
      " [0.00742231]\n",
      " [0.00739859]\n",
      " [0.9914435 ]\n",
      " [0.9914487 ]\n",
      " [0.00738637]\n",
      " [0.9914474 ]\n",
      " [0.99137366]\n",
      " [0.9914475 ]\n",
      " [0.0073814 ]\n",
      " [0.991446  ]\n",
      " [0.00738117]\n",
      " [0.9914339 ]\n",
      " [0.00738412]\n",
      " [0.00738552]\n",
      " [0.9914457 ]\n",
      " [0.99142665]\n",
      " [0.9914477 ]\n",
      " [0.9914416 ]\n",
      " [0.99144816]\n",
      " [0.9914435 ]\n",
      " [0.99144465]\n",
      " [0.9914443 ]\n",
      " [0.00738297]\n",
      " [0.9914463 ]\n",
      " [0.9914335 ]\n",
      " [0.99144876]\n",
      " [0.00738118]\n",
      " [0.9912627 ]\n",
      " [0.99144846]\n",
      " [0.9914441 ]\n",
      " [0.99144816]\n",
      " [0.00738198]\n",
      " [0.00741732]\n",
      " [0.00738981]\n",
      " [0.99144405]\n",
      " [0.00743538]\n",
      " [0.99144113]\n",
      " [0.9914473 ]\n",
      " [0.00738285]\n",
      " [0.991445  ]\n",
      " [0.99144787]\n",
      " [0.99143785]\n",
      " [0.9914485 ]\n",
      " [0.9914173 ]\n",
      " [0.00738138]\n",
      " [0.00738631]\n",
      " [0.9914427 ]\n",
      " [0.99144185]\n",
      " [0.00738402]\n",
      " [0.00740321]\n",
      " [0.00742256]\n",
      " [0.9914469 ]\n",
      " [0.9910658 ]\n",
      " [0.00750287]\n",
      " [0.9914486 ]\n",
      " [0.9914482 ]\n",
      " [0.00739009]\n",
      " [0.00740529]\n",
      " [0.9914473 ]\n",
      " [0.00738224]\n",
      " [0.00738191]\n",
      " [0.9914485 ]\n",
      " [0.00738882]\n",
      " [0.00738534]\n",
      " [0.007414  ]\n",
      " [0.00738492]\n",
      " [0.99144775]\n",
      " [0.99143946]\n",
      " [0.9914433 ]\n",
      " [0.9914483 ]\n",
      " [0.991448  ]\n",
      " [0.9914469 ]\n",
      " [0.00750918]\n",
      " [0.00738139]\n",
      " [0.991448  ]\n",
      " [0.9914486 ]\n",
      " [0.00738402]\n",
      " [0.00739113]\n",
      " [0.991448  ]\n",
      " [0.00738199]\n",
      " [0.991446  ]\n",
      " [0.9914482 ]\n",
      " [0.00738459]\n",
      " [0.00738234]\n",
      " [0.99144846]\n",
      " [0.99142116]\n",
      " [0.0073819 ]\n",
      " [0.0073821 ]\n",
      " [0.99143606]\n",
      " [0.00738391]\n",
      " [0.9914487 ]\n",
      " [0.9914487 ]\n",
      " [0.9913486 ]\n",
      " [0.00738879]\n",
      " [0.00738145]\n",
      " [0.9914469 ]\n",
      " [0.9914326 ]\n",
      " [0.00740353]\n",
      " [0.9914488 ]\n",
      " [0.9913346 ]\n",
      " [0.9914472 ]\n",
      " [0.99144876]\n",
      " [0.9914468 ]\n",
      " [0.99144584]\n",
      " [0.00740409]\n",
      " [0.00738255]\n",
      " [0.9914393 ]\n",
      " [0.9914483 ]\n",
      " [0.00738132]\n",
      " [0.00738257]\n",
      " [0.9914461 ]\n",
      " [0.9914473 ]\n",
      " [0.9914488 ]\n",
      " [0.00745078]\n",
      " [0.9914472 ]\n",
      " [0.9914463 ]\n",
      " [0.00738193]\n",
      " [0.9914483 ]\n",
      " [0.99144846]\n",
      " [0.00738158]\n",
      " [0.00739061]\n",
      " [0.99144864]\n",
      " [0.00738296]\n",
      " [0.00738182]\n",
      " [0.99144566]\n",
      " [0.0073812 ]\n",
      " [0.00739081]\n",
      " [0.9914488 ]\n",
      " [0.9914403 ]\n",
      " [0.9914473 ]\n",
      " [0.00738283]\n",
      " [0.00738884]\n",
      " [0.9914329 ]\n",
      " [0.9914472 ]\n",
      " [0.0073923 ]\n",
      " [0.00738914]\n",
      " [0.99143815]\n",
      " [0.9914277 ]\n",
      " [0.00738192]\n",
      " [0.99139136]\n",
      " [0.9914402 ]\n",
      " [0.99136853]\n",
      " [0.991441  ]\n",
      " [0.9914406 ]\n",
      " [0.9914331 ]\n",
      " [0.991441  ]\n",
      " [0.9914483 ]\n",
      " [0.9914475 ]\n",
      " [0.9914378 ]\n",
      " [0.00738317]\n",
      " [0.00738262]\n",
      " [0.9913727 ]\n",
      " [0.99144423]\n",
      " [0.00739427]\n",
      " [0.9914479 ]\n",
      " [0.00738974]\n",
      " [0.00738413]\n",
      " [0.9914487 ]\n",
      " [0.00738664]\n",
      " [0.0073811 ]\n",
      " [0.00738142]\n",
      " [0.00738255]\n",
      " [0.991416  ]\n",
      " [0.00739064]\n",
      " [0.00740047]\n",
      " [0.0073844 ]\n",
      " [0.00739325]\n",
      " [0.99131966]\n",
      " [0.99141985]\n",
      " [0.00738227]\n",
      " [0.00738131]\n",
      " [0.9914398 ]\n",
      " [0.99143916]\n",
      " [0.00738162]\n",
      " [0.00740679]\n",
      " [0.00738356]\n",
      " [0.9914473 ]\n",
      " [0.99137557]\n",
      " [0.00741111]\n",
      " [0.00738429]\n",
      " [0.00738138]\n",
      " [0.00738184]\n",
      " [0.00738138]\n",
      " [0.9914474 ]\n",
      " [0.9914486 ]\n",
      " [0.9913814 ]\n",
      " [0.9914482 ]\n",
      " [0.00739602]\n",
      " [0.00740757]\n",
      " [0.00738643]\n",
      " [0.9914435 ]\n",
      " [0.99144274]\n",
      " [0.99144804]\n",
      " [0.00738339]\n",
      " [0.9914404 ]\n",
      " [0.00738221]\n",
      " [0.9914481 ]\n",
      " [0.99144727]\n",
      " [0.00738201]\n",
      " [0.99139243]\n",
      " [0.00739251]\n",
      " [0.00739273]\n",
      " [0.00739387]\n",
      " [0.9914308 ]\n",
      " [0.9914378 ]\n",
      " [0.9914472 ]\n",
      " [0.00738295]\n",
      " [0.9914434 ]\n",
      " [0.991448  ]\n",
      " [0.00738241]\n",
      " [0.9914359 ]\n",
      " [0.00738487]\n",
      " [0.00738584]\n",
      " [0.99144834]\n",
      " [0.00738383]\n",
      " [0.9914469 ]\n",
      " [0.00738186]\n",
      " [0.00740837]\n",
      " [0.00738201]\n",
      " [0.00742451]\n",
      " [0.00738451]\n",
      " [0.99143726]\n",
      " [0.9914474 ]\n",
      " [0.9914317 ]\n",
      " [0.9914478 ]\n",
      " [0.00738447]\n",
      " [0.99137443]\n",
      " [0.9914118 ]\n",
      " [0.007389  ]\n",
      " [0.00738892]\n",
      " [0.9914473 ]\n",
      " [0.99144834]\n",
      " [0.99144566]\n",
      " [0.00749411]\n",
      " [0.99143213]\n",
      " [0.99144423]\n",
      " [0.00738331]\n",
      " [0.00740873]\n",
      " [0.99141526]\n",
      " [0.9914483 ]\n",
      " [0.99144787]\n",
      " [0.00738583]\n",
      " [0.9914479 ]\n",
      " [0.9914188 ]\n",
      " [0.00738229]\n",
      " [0.00738434]\n",
      " [0.00738205]\n",
      " [0.9914472 ]\n",
      " [0.00738139]\n",
      " [0.00740199]\n",
      " [0.9914457 ]\n",
      " [0.00738951]\n",
      " [0.00738124]\n",
      " [0.9914316 ]\n",
      " [0.99144155]\n",
      " [0.00738831]\n",
      " [0.99144816]\n",
      " [0.0073817 ]\n",
      " [0.00738141]\n",
      " [0.99126935]\n",
      " [0.99144864]\n",
      " [0.99144775]\n",
      " [0.99144644]\n",
      " [0.0073811 ]\n",
      " [0.99140185]\n",
      " [0.99143904]\n",
      " [0.9914462 ]\n",
      " [0.00739809]\n",
      " [0.9914436 ]\n",
      " [0.99144477]\n",
      " [0.00739274]\n",
      " [0.00742936]\n",
      " [0.99141526]\n",
      " [0.00738174]\n",
      " [0.9913562 ]\n",
      " [0.99143404]\n",
      " [0.99128544]\n",
      " [0.00745772]\n",
      " [0.00739273]\n",
      " [0.99144745]\n",
      " [0.00738302]\n",
      " [0.00738812]\n",
      " [0.00738488]\n",
      " [0.00738188]\n",
      " [0.00738817]\n",
      " [0.99144864]\n",
      " [0.00741236]\n",
      " [0.00738121]\n",
      " [0.9914477 ]\n",
      " [0.00738139]\n",
      " [0.9914166 ]\n",
      " [0.9914486 ]\n",
      " [0.00738417]\n",
      " [0.00738489]\n",
      " [0.00738892]\n",
      " [0.00738803]\n",
      " [0.00738264]\n",
      " [0.00738247]\n",
      " [0.00738555]\n",
      " [0.991418  ]\n",
      " [0.00738259]\n",
      " [0.00738319]\n",
      " [0.9914402 ]\n",
      " [0.9913272 ]\n",
      " [0.9914459 ]\n",
      " [0.007382  ]\n",
      " [0.00738207]\n",
      " [0.00738369]\n",
      " [0.00742108]\n",
      " [0.00742573]\n",
      " [0.99144673]\n",
      " [0.00739815]\n",
      " [0.99144757]\n",
      " [0.99124056]\n",
      " [0.00738376]\n",
      " [0.00738334]\n",
      " [0.9914477 ]\n",
      " [0.9914412 ]\n",
      " [0.9914441 ]\n",
      " [0.00738146]\n",
      " [0.0074079 ]\n",
      " [0.00738214]\n",
      " [0.9914401 ]\n",
      " [0.0074353 ]\n",
      " [0.0073813 ]\n",
      " [0.9914461 ]\n",
      " [0.00738301]\n",
      " [0.00739376]\n",
      " [0.9914418 ]\n",
      " [0.00738999]\n",
      " [0.00738197]\n",
      " [0.00738244]\n",
      " [0.00738147]\n",
      " [0.00738533]\n",
      " [0.99144584]\n",
      " [0.00738115]\n",
      " [0.9914448 ]\n",
      " [0.9914464 ]\n",
      " [0.99144876]\n",
      " [0.99144685]\n",
      " [0.9914475 ]\n",
      " [0.9914433 ]\n",
      " [0.00744468]\n",
      " [0.9914479 ]\n",
      " [0.9914486 ]\n",
      " [0.00738308]\n",
      " [0.9914119 ]\n",
      " [0.9914309 ]\n",
      " [0.9914478 ]\n",
      " [0.9914477 ]\n",
      " [0.00738434]\n",
      " [0.00738558]\n",
      " [0.00738574]\n",
      " [0.00738633]\n",
      " [0.99142766]\n",
      " [0.9914442 ]\n",
      " [0.00738124]\n",
      " [0.00738229]\n",
      " [0.00739153]\n",
      " [0.00738137]\n",
      " [0.00744779]\n",
      " [0.00739156]\n",
      " [0.99144524]\n",
      " [0.00738115]\n",
      " [0.00738254]\n",
      " [0.00738766]\n",
      " [0.0073883 ]\n",
      " [0.9914416 ]\n",
      " [0.99144393]\n",
      " [0.00738116]\n",
      " [0.0073828 ]\n",
      " [0.0073847 ]\n",
      " [0.9914482 ]\n",
      " [0.00738113]\n",
      " [0.00740365]\n",
      " [0.00738148]\n",
      " [0.00738267]\n",
      " [0.00739545]\n",
      " [0.99144745]\n",
      " [0.9914475 ]\n",
      " [0.9914365 ]\n",
      " [0.99144775]\n",
      " [0.00742682]\n",
      " [0.00738116]\n",
      " [0.00738182]\n",
      " [0.99144477]\n",
      " [0.99144065]\n",
      " [0.99144864]\n",
      " [0.9914452 ]\n",
      " [0.00738286]\n",
      " [0.9914478 ]\n",
      " [0.9914486 ]\n",
      " [0.00738606]\n",
      " [0.00738131]\n",
      " [0.00738154]\n",
      " [0.00738177]\n",
      " [0.9914468 ]\n",
      " [0.99142647]\n",
      " [0.99144715]\n",
      " [0.99143916]\n",
      " [0.00739231]\n",
      " [0.00738828]\n",
      " [0.9914478 ]\n",
      " [0.99144757]\n",
      " [0.99133724]\n",
      " [0.9914483 ]\n",
      " [0.0073936 ]\n",
      " [0.00738546]\n",
      " [0.0073921 ]\n",
      " [0.00738133]\n",
      " [0.00739152]\n",
      " [0.00739589]\n",
      " [0.99144685]\n",
      " [0.00738309]\n",
      " [0.00758358]\n",
      " [0.9914416 ]\n",
      " [0.99144804]\n",
      " [0.00741643]\n",
      " [0.9914486 ]\n",
      " [0.99144346]\n",
      " [0.9914182 ]\n",
      " [0.00744575]\n",
      " [0.9914474 ]\n",
      " [0.00738138]\n",
      " [0.99144846]\n",
      " [0.9914481 ]\n",
      " [0.99144334]\n",
      " [0.00738123]\n",
      " [0.99144816]\n",
      " [0.9914485 ]\n",
      " [0.9914406 ]\n",
      " [0.99144554]\n",
      " [0.00738351]\n",
      " [0.00738439]\n",
      " [0.00745381]\n",
      " [0.9914486 ]\n",
      " [0.9914487 ]\n",
      " [0.00738128]\n",
      " [0.99144375]\n",
      " [0.99144596]\n",
      " [0.9914445 ]\n",
      " [0.99144244]\n",
      " [0.00745025]\n",
      " [0.00738211]\n",
      " [0.00738243]\n",
      " [0.00738127]\n",
      " [0.00738726]\n",
      " [0.9914383 ]\n",
      " [0.00738922]\n",
      " [0.00738466]\n",
      " [0.00738343]\n",
      " [0.99144614]\n",
      " [0.9914465 ]\n",
      " [0.00740099]\n",
      " [0.00738129]\n",
      " [0.00738247]\n",
      " [0.99143296]\n",
      " [0.00738225]\n",
      " [0.00738565]\n",
      " [0.99144113]\n",
      " [0.00738135]\n",
      " [0.9914477 ]\n",
      " [0.00738163]\n",
      " [0.00738681]\n",
      " [0.00739335]\n",
      " [0.0073916 ]\n",
      " [0.99144834]\n",
      " [0.00738485]\n",
      " [0.9914449 ]\n",
      " [0.0073817 ]\n",
      " [0.00738734]\n",
      " [0.00738164]\n",
      " [0.99143326]\n",
      " [0.00738128]\n",
      " [0.00740779]\n",
      " [0.00738847]\n",
      " [0.99144876]\n",
      " [0.9914485 ]\n",
      " [0.99144304]\n",
      " [0.9914266 ]\n",
      " [0.00738151]\n",
      " [0.00738676]\n",
      " [0.99144846]\n",
      " [0.9914264 ]\n",
      " [0.9912544 ]\n",
      " [0.9914448 ]\n",
      " [0.99144787]\n",
      " [0.99144804]\n",
      " [0.99142855]\n",
      " [0.00738245]\n",
      " [0.00738848]\n",
      " [0.9914402 ]\n",
      " [0.9914221 ]\n",
      " [0.9914427 ]\n",
      " [0.991448  ]\n",
      " [0.9914487 ]\n",
      " [0.0073897 ]\n",
      " [0.00738109]\n",
      " [0.00738159]\n",
      " [0.00738161]\n",
      " [0.9914389 ]\n",
      " [0.99144065]\n",
      " [0.00738398]\n",
      " [0.99144673]\n",
      " [0.991446  ]\n",
      " [0.9914312 ]\n",
      " [0.9914425 ]\n",
      " [0.99144375]\n",
      " [0.99130535]\n",
      " [0.00738295]\n",
      " [0.00738299]\n",
      " [0.00740511]\n",
      " [0.00738148]\n",
      " [0.00738161]\n",
      " [0.99144834]\n",
      " [0.00738191]\n",
      " [0.0073924 ]\n",
      " [0.9914351 ]\n",
      " [0.00738137]\n",
      " [0.9914461 ]\n",
      " [0.99144554]\n",
      " [0.99144715]\n",
      " [0.9914488 ]\n",
      " [0.00738193]\n",
      " [0.00738804]\n",
      " [0.99134904]\n",
      " [0.99144244]\n",
      " [0.00738116]\n",
      " [0.00738123]\n",
      " [0.99140877]\n",
      " [0.00738411]\n",
      " [0.99119365]\n",
      " [0.00738541]\n",
      " [0.9914449 ]\n",
      " [0.9914484 ]\n",
      " [0.991448  ]\n",
      " [0.00738303]\n",
      " [0.99144304]\n",
      " [0.9914476 ]\n",
      " [0.0073823 ]\n",
      " [0.00742374]\n",
      " [0.00799736]\n",
      " [0.00738627]\n",
      " [0.00738213]\n",
      " [0.99144685]\n",
      " [0.00738302]\n",
      " [0.00738108]\n",
      " [0.00738361]\n",
      " [0.00738144]\n",
      " [0.0073839 ]\n",
      " [0.9914474 ]\n",
      " [0.9914422 ]\n",
      " [0.9914462 ]\n",
      " [0.00739223]\n",
      " [0.00741685]\n",
      " [0.00738265]\n",
      " [0.00743868]\n",
      " [0.00742532]\n",
      " [0.00739084]\n",
      " [0.00739273]\n",
      " [0.99138767]\n",
      " [0.00738677]\n",
      " [0.99144846]\n",
      " [0.9914397 ]\n",
      " [0.00738134]\n",
      " [0.00744336]\n",
      " [0.99144644]\n",
      " [0.00738132]\n",
      " [0.00741105]\n",
      " [0.99144834]\n",
      " [0.99144775]\n",
      " [0.99143624]\n",
      " [0.99144596]\n",
      " [0.00738767]\n",
      " [0.00738557]\n",
      " [0.0073919 ]\n",
      " [0.007458  ]\n",
      " [0.00738148]\n",
      " [0.9914411 ]\n",
      " [0.00738518]\n",
      " [0.99143744]\n",
      " [0.99127084]\n",
      " [0.0073894 ]\n",
      " [0.9914461 ]\n",
      " [0.00739233]\n",
      " [0.00738148]\n",
      " [0.99144864]\n",
      " [0.9914302 ]\n",
      " [0.99141675]\n",
      " [0.00738408]\n",
      " [0.99144536]\n",
      " [0.99144787]\n",
      " [0.00738495]\n",
      " [0.99144644]\n",
      " [0.00738171]\n",
      " [0.9914302 ]\n",
      " [0.9914487 ]\n",
      " [0.9911745 ]\n",
      " [0.99142593]\n",
      " [0.0073857 ]\n",
      " [0.9914466 ]\n",
      " [0.9914476 ]\n",
      " [0.00738151]\n",
      " [0.9914469 ]\n",
      " [0.99144846]\n",
      " [0.9914396 ]\n",
      " [0.00738244]\n",
      " [0.9914033 ]\n",
      " [0.00738257]\n",
      " [0.9914377 ]\n",
      " [0.00738752]\n",
      " [0.00738112]\n",
      " [0.00738555]\n",
      " [0.00738803]\n",
      " [0.99143183]\n",
      " [0.00738323]\n",
      " [0.9914434 ]\n",
      " [0.00738565]\n",
      " [0.99139684]\n",
      " [0.0073812 ]\n",
      " [0.00738189]\n",
      " [0.00742184]\n",
      " [0.00738244]\n",
      " [0.00738179]\n",
      " [0.00741993]\n",
      " [0.99144816]\n",
      " [0.00738121]\n",
      " [0.00738471]\n",
      " [0.9914428 ]\n",
      " [0.00738833]\n",
      " [0.9914483 ]\n",
      " [0.00740039]\n",
      " [0.00738262]\n",
      " [0.99143785]\n",
      " [0.9914445 ]\n",
      " [0.00738497]\n",
      " [0.00739224]\n",
      " [0.00738258]\n",
      " [0.00738943]\n",
      " [0.9914453 ]\n",
      " [0.0073813 ]\n",
      " [0.9914395 ]\n",
      " [0.9914356 ]\n",
      " [0.00745542]\n",
      " [0.9914395 ]\n",
      " [0.00738145]\n",
      " [0.9914448 ]\n",
      " [0.9914476 ]\n",
      " [0.9914445 ]\n",
      " [0.00742343]\n",
      " [0.9913441 ]\n",
      " [0.00738144]\n",
      " [0.9914446 ]\n",
      " [0.00738168]\n",
      " [0.00742071]\n",
      " [0.9914274 ]\n",
      " [0.00739302]\n",
      " [0.991442  ]\n",
      " [0.99143726]\n",
      " [0.00738391]\n",
      " [0.00738111]\n",
      " [0.99144405]\n",
      " [0.00738401]\n",
      " [0.99144816]\n",
      " [0.00738145]\n",
      " [0.00766732]\n",
      " [0.99144846]\n",
      " [0.9913484 ]\n",
      " [0.00738373]\n",
      " [0.9914454 ]\n",
      " [0.00739623]\n",
      " [0.99143505]\n",
      " [0.00740266]\n",
      " [0.0073841 ]\n",
      " [0.99144506]\n",
      " [0.99144816]\n",
      " [0.00738698]\n",
      " [0.0074267 ]\n",
      " [0.99140966]\n",
      " [0.9914415 ]\n",
      " [0.00739116]\n",
      " [0.00738508]\n",
      " [0.9913684 ]\n",
      " [0.991438  ]\n",
      " [0.99127126]\n",
      " [0.9914206 ]\n",
      " [0.99144644]\n",
      " [0.00738262]\n",
      " [0.00738731]\n",
      " [0.00738124]\n",
      " [0.00738122]\n",
      " [0.00738124]\n",
      " [0.9913815 ]\n",
      " [0.99144584]\n",
      " [0.00738282]\n",
      " [0.99134547]\n",
      " [0.00754527]\n",
      " [0.00738533]\n",
      " [0.00738239]\n",
      " [0.00738228]\n",
      " [0.9913733 ]\n",
      " [0.9914481 ]\n",
      " [0.00738366]\n",
      " [0.99144834]\n",
      " [0.99144447]\n",
      " [0.99144274]\n",
      " [0.00738369]\n",
      " [0.9914338 ]\n",
      " [0.00740781]\n",
      " [0.00738375]\n",
      " [0.00738229]\n",
      " [0.991415  ]\n",
      " [0.99141157]\n",
      " [0.00738118]\n",
      " [0.00738134]\n",
      " [0.9914452 ]\n",
      " [0.9914467 ]\n",
      " [0.00738964]\n",
      " [0.9914402 ]\n",
      " [0.9914447 ]\n",
      " [0.00738331]\n",
      " [0.9914473 ]\n",
      " [0.9914476 ]\n",
      " [0.99142814]\n",
      " [0.0073828 ]\n",
      " [0.0073956 ]\n",
      " [0.99144876]\n",
      " [0.9914484 ]\n",
      " [0.00738625]\n",
      " [0.00774528]\n",
      " [0.99144566]\n",
      " [0.00738395]\n",
      " [0.00738115]\n",
      " [0.00740381]\n",
      " [0.9914394 ]\n",
      " [0.99144584]\n",
      " [0.00738146]\n",
      " [0.00740507]\n",
      " [0.00738183]\n",
      " [0.00738228]\n",
      " [0.9913063 ]\n",
      " [0.9914477 ]\n",
      " [0.99144226]\n",
      " [0.9914069 ]\n",
      " [0.00738139]\n",
      " [0.00738171]\n",
      " [0.0073815 ]\n",
      " [0.0073841 ]\n",
      " [0.00738715]\n",
      " [0.00738266]\n",
      " [0.00738111]\n",
      " [0.00739191]\n",
      " [0.00738417]\n",
      " [0.00739902]\n",
      " [0.00741067]\n",
      " [0.0073833 ]\n",
      " [0.99143285]\n",
      " [0.00738573]\n",
      " [0.99144626]\n",
      " [0.99143547]\n",
      " [0.00738428]\n",
      " [0.00738132]\n",
      " [0.99143064]\n",
      " [0.99144804]\n",
      " [0.00738131]\n",
      " [0.00738435]\n",
      " [0.00740418]\n",
      " [0.99136615]\n",
      " [0.9914482 ]\n",
      " [0.99144757]\n",
      " [0.9914449 ]\n",
      " [0.00741187]\n",
      " [0.00738131]\n",
      " [0.0073864 ]\n",
      " [0.99144346]\n",
      " [0.0073831 ]\n",
      " [0.9914481 ]\n",
      " [0.00738633]\n",
      " [0.00738413]\n",
      " [0.9914335 ]\n",
      " [0.00739002]\n",
      " [0.99144775]\n",
      " [0.00739661]\n",
      " [0.9914476 ]\n",
      " [0.9914486 ]\n",
      " [0.99144477]\n",
      " [0.99144864]\n",
      " [0.9914479 ]\n",
      " [0.9914477 ]\n",
      " [0.99139297]\n",
      " [0.00739682]\n",
      " [0.00738917]\n",
      " [0.00738758]\n",
      " [0.99144834]\n",
      " [0.00738163]\n",
      " [0.9914213 ]\n",
      " [0.9914418 ]\n",
      " [0.9914011 ]\n",
      " [0.00738262]\n",
      " [0.00738415]\n",
      " [0.00738221]\n",
      " [0.99144757]\n",
      " [0.00743811]\n",
      " [0.0074013 ]\n",
      " [0.00738123]\n",
      " [0.00738659]\n",
      " [0.99138397]\n",
      " [0.9914446 ]\n",
      " [0.99144816]\n",
      " [0.99144727]\n",
      " [0.99144757]\n",
      " [0.00738116]\n",
      " [0.0073982 ]\n",
      " [0.00738113]\n",
      " [0.007386  ]\n",
      " [0.9914209 ]\n",
      " [0.00738768]\n",
      " [0.00738319]\n",
      " [0.00738863]\n",
      " [0.99144876]\n",
      " [0.9914455 ]\n",
      " [0.00738255]\n",
      " [0.99144703]\n",
      " [0.00739252]\n",
      " [0.00738734]\n",
      " [0.9914485 ]\n",
      " [0.99144137]\n",
      " [0.00738339]\n",
      " [0.9914055 ]\n",
      " [0.9914468 ]\n",
      " [0.00739099]\n",
      " [0.99144673]\n",
      " [0.99143934]\n",
      " [0.0073812 ]\n",
      " [0.00738292]\n",
      " [0.99142945]\n",
      " [0.99144584]\n",
      " [0.00738154]\n",
      " [0.99144703]\n",
      " [0.9914487 ]\n",
      " [0.007386  ]\n",
      " [0.00739011]\n",
      " [0.99144614]\n",
      " [0.9914487 ]\n",
      " [0.0073893 ]\n",
      " [0.00738678]\n",
      " [0.9914481 ]\n",
      " [0.9914476 ]\n",
      " [0.00738346]\n",
      " [0.99144316]\n",
      " [0.00738415]\n",
      " [0.9914484 ]\n",
      " [0.00738178]\n",
      " [0.99144703]\n",
      " [0.9914461 ]\n",
      " [0.9914394 ]\n",
      " [0.9914458 ]\n",
      " [0.9914486 ]\n",
      " [0.00739679]\n",
      " [0.00738407]\n",
      " [0.00738205]\n",
      " [0.00738308]\n",
      " [0.00738949]\n",
      " [0.9914486 ]\n",
      " [0.9914349 ]\n",
      " [0.99144566]\n",
      " [0.00739074]\n",
      " [0.00738163]\n",
      " [0.99139935]\n",
      " [0.9914462 ]\n",
      " [0.00738126]\n",
      " [0.9914486 ]\n",
      " [0.00749516]\n",
      " [0.9914379 ]\n",
      " [0.99144244]\n",
      " [0.991414  ]\n",
      " [0.00738378]\n",
      " [0.00739566]\n",
      " [0.00738188]\n",
      " [0.9914477 ]\n",
      " [0.9914479 ]\n",
      " [0.9914251 ]\n",
      " [0.9914323 ]\n",
      " [0.00740021]\n",
      " [0.00738415]\n",
      " [0.9913014 ]\n",
      " [0.00739309]\n",
      " [0.99144864]\n",
      " [0.00739502]\n",
      " [0.0073907 ]\n",
      " [0.9914479 ]\n",
      " [0.99144137]\n",
      " [0.99138623]\n",
      " [0.9914478 ]\n",
      " [0.00738505]\n",
      " [0.0073819 ]\n",
      " [0.00738196]\n",
      " [0.00739769]\n",
      " [0.9914486 ]\n",
      " [0.9914293 ]\n",
      " [0.99144846]\n",
      " [0.00738359]\n",
      " [0.00739597]\n",
      " [0.00738375]\n",
      " [0.007398  ]\n",
      " [0.00738195]\n",
      " [0.0073884 ]\n",
      " [0.991448  ]\n",
      " [0.991438  ]\n",
      " [0.00741204]\n",
      " [0.00738236]]\n"
     ]
    }
   ],
   "source": [
    "a_3 = modelo.predict(X_train)\n",
    "print(f\"As previsões para o valor da probabilidade de x_1 pertencer à classe 1 são: \\n{a_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_treinamento = (a_3 >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conta=0\n",
    "soma_erro = 0\n",
    "\n",
    "for i in range (len(y_previsto)):\n",
    "    if y_previsto[i] == y_train[i]: \n",
    "        conta +=1\n",
    "\n",
    "conta/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um conjunto de teste da mesma maneira que criamos o de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "classes = 2\n",
    "m = 200\n",
    "centers = [[-5, -5, -5], [5, 5, 5]]\n",
    "std = 2.0\n",
    "X_test, y_test = make_blobs(n_samples=m, centers=centers, cluster_std=std,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "a_3 = modelo.predict(X_test)\n",
    "previsoes_testes = (a_3 >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conta=0\n",
    "\n",
    "for i in range (len(previsoes_testes)):\n",
    "    if previsoes_testes[i] == y_test[i]: \n",
    "        conta +=1\n",
    "\n",
    "conta/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear: $$a=g(z)=z$$\n",
    "\n",
    "- Sigmoid: $$a = g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "- Relu: $$a = g(z)= \\max(0,z)$$\n",
    "\n",
    "- Leaky ReKY: $$a = g(z)= \\max(0.1z , z)$$\n",
    "\n",
    "- Softmax (multiclasse, com $K$ classes): $$a = g(z_i) = \\frac{e^{z_i}}{\\sum_j^Ke^{z_j}}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo do uso da função de ativação Softmax:**\n",
    "\n",
    "<img src = \"softmax.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguinte notação será utilizada para uma rede neural multicamada:\n",
    "\n",
    "- O número de camadas na rede é dado por $L$ e o número de neurônios em uma camada $l$ é dado por $n^{[l]}$ , sendo $n^{[0]}=n$ o número de atributos.\n",
    "- Um exemplo de treinamento é representado por um vetor coluna de $n$  atributos.\n",
    "\n",
    "$$ \\vec{x}^{(i)} \\in \\mathbb{R}^{n^{[0]}} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]^T $$\n",
    "\n",
    "- A matriz $ \\mathbf{X} \\in \\mathbb{R}^{n^{[0]} \\times m}$ organiza cada um dos $m$ exemplos de treinamento em uma coluna, assim, $\\mathbf{X}_{:,i} = x^{(i)}$.\n",
    "\n",
    "- $y^{(i)}$ representa um rótulo atribuído ao exemplo $\\vec{x}^{(i)}$. O vetor $\\vec{y} = [y^{(i)},…,y^{(m)}] $ representa os rótulos do conjunto de treinamento. \n",
    "\n",
    "- Uma matriz $\\mathbf{W}^{[l]} \\in \\mathbb{R}^{n^{[l-1]} \\times n^{[l]}}$ representa os pesos utilizados para ponderar a entrada de uma camada $l$ com uma equação linear do tipo: $w \\cdot x+b$. \n",
    "\n",
    "- A entrada da matriz na linha $i$ e coluna $j$, $W_{i,j}^{[l]}$ , representa o peso associado a uma entrada no neurônio $j$  da camada $l$ que corresponde a uma saída do neurônio $i$ da camada $(l-1)$. \n",
    "\n",
    "- O vetor coluna $\\vec{b}^{[l]}$  recebe os valores dos interceptos das $n^{[l]}$  equações lineares correspondentes às entradas dos neurônios da camada $l$.\n",
    "\n",
    "- $\\vec{z}^{[l]} = \\mathbf{W}^{[l]^T} \\cdot \\vec{a}^{[l-1]} + \\vec{b}^{[l]}$, onde $\\vec{z}^{[l]} \\in \\mathbb{R}^{n^{[l]}}$, representa um vetor coluna com a ponderação das entradas nos neurônios da camada $l$. Na primeira camada intermediária, o exemplo de treinamento é ponderado, $ \\vec{a}^{[0]} = \\vec{x}$.\n",
    "\n",
    "- Uma matriz $\\mathbf{Z}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m} $ representa as ponderações das entradas para os $m$ diferentes exemplos do conjunto de treinamento Dessa forma: $\\mathbf{A}^{[0]} = \\mathbf{X}$. \n",
    "\n",
    "- $\\vec{a}^{[l]} \\in \\mathbb{R}^{n^{[l]}}$ representa a saída da função de ativação $g^[l]$ aplicada na camada $l$. Assim, para uma camada $l$ temos: $\\vec{a}^{[l]}=g^{[l]} (\\vec{z}^{[l]})$.\n",
    "\n",
    "- Uma matriz $\\mathbf{A}^{[l]} \\in \\mathbb{R}^{n^{[l]} \\times m}$ representa as saídas das funções de ativação em cada camada $l$ (linhas) para os $m$ exemplos de treinamento (colunas).\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
