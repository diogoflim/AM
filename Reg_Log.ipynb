{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875a6780-dc7d-4fc2-8037-8bd7e8f78f58",
   "metadata": {},
   "source": [
    "**TPP - UFF**\n",
    "\n",
    "**Aprendizado de Máquina e Decisões Dirigidas por Dados**\n",
    "\n",
    "Docente: Diogo Ferreira de Lima Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e401dd-993b-4d73-9803-76dcbab38b44",
   "metadata": {},
   "source": [
    "# Lembrando as Funções da Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73cb62-d57a-4fd6-b6ca-fc4ff351b234",
   "metadata": {},
   "source": [
    "A Regressão Logística supõe:\n",
    "\n",
    "- **Problema de classificação binária**: temos $y_i \\in \\mathcal{C} =\\{0,1\\}$\n",
    "\n",
    "- $\\hat{f}(\\mathbf{x}_i)= \\hat{y}_i \\approx y_i=P(y_i=1|\\mathbf{x}_i)$\n",
    "\n",
    "Da segunda suposição, temos que a função que aprenderemos a partir dos dados deve estimar a probabilidade de uma entrada $\\mathbf{x}_i$ pertencer à classe 1. Nesse cenário, precisamos que:\n",
    "\n",
    "$$0<=\\hat{y}_i<=1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6e7bf-6c3e-49e1-8233-29bf7e4b3232",
   "metadata": {},
   "source": [
    "Assim, não podemos seguir com a mesma lógica do Perceptron, onde a classe de um exemplo era obtida a partir do sinal da equação $\\mathbf{w}^T \\mathbf{x}$. \n",
    "\n",
    "Agora, devemos aprender uma função que retorna a probabilidade acima. Para isso, usamos a função sigmoide.\n",
    "\n",
    "$$\\theta(\\mathbf{w}^T \\mathbf{x}) = \\frac{1}{1+ e^{-\\mathbf{w}^T \\mathbf{x}}}$$\n",
    "\n",
    "ou, usando $z = \\mathbf{w}^T \\mathbf(x)$:\n",
    "\n",
    "$$ \\theta(z)=\\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d55038-b825-4689-aadb-04097c41021d",
   "metadata": {},
   "source": [
    "## Implementando a Função sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c4a5b-a552-41a5-80e6-99e9a59e8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sigmoid = lambda z: 1/(1 + np.exp(-z)) # função sigmoid\n",
    "\n",
    "print ('Sigmoid(-1) = ', sigmoid(-1))\n",
    "print ('Sigmoid(0) = ', sigmoid(0))\n",
    "print ('Sigmoid(1) = ', sigmoid(1))\n",
    "print ('Sigmoid(10) = ', sigmoid(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d03cf-2362-4005-b309-651cf2729c79",
   "metadata": {},
   "source": [
    "## Visualizando a função sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf272c9c-4d4c-4f1f-9aad-f3aef180f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar um vetor de 100 pontos linearmente espaçados entre -20 e 20\n",
    " \n",
    "x = np.linspace(-20, 20, 100)  \n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cedc1-1c46-49dd-b504-9ddb63659b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1d0dd-97db-411e-9255-a9846b84e745",
   "metadata": {},
   "source": [
    "Seja $z = \\theta(\\mathbf{w}^T \\mathbf{x})$. Temos:\n",
    "\n",
    "- Se $z\\rightarrow\\infty$, então $\\theta(z) \\approx \\frac{1}{1+0}=1$\n",
    "\n",
    "- Se $z\\rightarrow - \\infty$, então $\\theta(z) \\approx \\frac{1}{1+\\infty}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c9578-c010-4e4a-b279-a284b9eb4ac1",
   "metadata": {},
   "source": [
    "## Função Perda e Função Custo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c56f46-bf73-48d9-b8e5-2184c9c526b1",
   "metadata": {},
   "source": [
    "**Chegamos então a nossa suposição:**\n",
    "\n",
    "$$\\hat{f}(x)=\\hat{y}=\\theta(\\mathbf{w}^T \\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2b7a7-0b83-4d7c-902c-56775257a02a",
   "metadata": {},
   "source": [
    "Nossa função pode ser entendida como uma função de ativação que será aplicada tal como o esquema a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a091de8-1732-4cec-ace8-52c56e1ed09c",
   "metadata": {},
   "source": [
    "![](esquema.png)\n",
    "\n",
    "fonte da imagem: https://sebastianraschka.com/faq/docs/logisticregr-neuralnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b19a0-1dc9-42e3-af64-c79e3576a3de",
   "metadata": {},
   "source": [
    "**A nossa tarefa é, então, aprender o vetor de parâmetros $\\mathbf{w}$. Em Regressão Logística, fazemos isso minimizando uma função custo específica.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6737ef5-c151-446d-9d4a-32266e8429ac",
   "metadata": {},
   "source": [
    "### Entendendo a Função Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778e894-64a1-4dff-b8f1-6a1347239781",
   "metadata": {},
   "source": [
    "A perda para uma determinada observação $\\mathbf{x}_i$ é dada por:\n",
    "\n",
    "$$ L(\\hat{y}_i, y_i) = -[y_i\\times\\log{\\hat{y}_i + (1-y_i)\\times\\log{(1-\\hat{y}_i})}]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc31ff-e797-4fc8-a160-25594ff24c27",
   "metadata": {},
   "source": [
    " Podemos observar que essa função faz sentido analisando os 2 casos a seguir:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd82d5-cb42-431d-b61f-9c42e177bacf",
   "metadata": {},
   "source": [
    "- **Caso 1:** $y_i=1$:\n",
    "$$ L(\\hat{y}_i, 1) = -[1 \\times \\log{\\hat{y}_i + (1-1)\\times\\log{(1-\\hat{y}_i})}]$$\n",
    "Ou seja:\n",
    "$$ L(\\hat{y}_i, 1) = -\\log{\\hat{y}_i}$$\n",
    "**Interpretação do caso 1**: Para minimizar a perda, queremos que $\\hat{y}_i$ seja o maior possível, assim, próximo de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06ac79-95a8-436f-a896-b0890088639e",
   "metadata": {},
   "source": [
    "- **Caso 2:** $y_i=0$:\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}_i, 0) = -[0 \\times \\log{\\hat{y}_i + (1-0)\\times\\log{(1-\\hat{y}_i})}]$$\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}_i, 0) = -\\log{(1-\\hat{y}_i)}$$\n",
    "\n",
    "**Interpretação do caso 2**: Para minimizar a perda, queremos que $\\hat{y}_i$ seja o menor possível, assim, próximo de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34eb9e2-2ece-4677-a9e5-0b4c799240f6",
   "metadata": {},
   "source": [
    "### Entendendo a Função Custo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044d65f-0808-4c74-9eb7-167ca7bfab4f",
   "metadata": {},
   "source": [
    "Por sua vez, a função custo é dada pela média das perdas obtidas com os $n$ exemplos de treinamento. Dessa forma, temos:\n",
    "\n",
    "$$ J = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}(\\hat{y}_i, y_i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c76fea-25e5-4fb4-a674-c5382dc7fefe",
   "metadata": {},
   "source": [
    "## Gradiente Descendente\n",
    "\n",
    "### Aprendendo o vetor $\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b26b0-ee33-44af-a96e-dda01235b329",
   "metadata": {},
   "source": [
    "Para o treinamento, nos deparamos com um conjunto de dados do seguinte tipo:\n",
    "    \n",
    "$$\\mathcal{D}=\\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),...,(\\mathbf{x}_n,y_n)\\}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27408af-54cf-465e-843f-7dcdd3d0fee3",
   "metadata": {},
   "source": [
    "Como visto em aula, podemos denotar o conjunto de treinamento com uma matriz de atributos $\\mathbf{X}$ e um vetor de rótulos $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae71e6f-e13c-45b8-a4c7-4d8991c4e552",
   "metadata": {},
   "source": [
    "A ideia do gradiente descendente é atualizar os parâmetros a cada passo do algoritmo conforme for a direção do gradiente da função custo em termos desses parâmetros. \n",
    "\n",
    "Temos:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\times \\frac{\\partial J}{\\partial w_j}$$ \n",
    "\n",
    "- onde $\\alpha$ consiste em uma taxa de aprendizagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56e413",
   "metadata": {},
   "source": [
    "\n",
    "![](gradiente_descendente.png)\n",
    "\n",
    "Fonte da imagem: \n",
    "https://medium.com/data-hackers/gradientes-descendentes-na-pr%C3%A1tica-melhor-jeito-de-entender-740ef4ff6c43\n",
    "\n",
    "\n",
    "**A ideia é a seguinte:**\n",
    "\n",
    "- Se a derivada parcial é positiva, estamos \"à direita\" do mínimo e, assim, devemos reduzir o valor de $w_j$.\n",
    "- Se a derivada parcial é negativa, estamos \"à esquerda\" do mínimo e, assim, devemos reduzir o valor de $w_j$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0213d",
   "metadata": {},
   "source": [
    "### Pergunta: Qual o valor da derivada?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762b002",
   "metadata": {},
   "source": [
    "$$dw=\\frac{\\partial J}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16271fda",
   "metadata": {},
   "source": [
    "Para chegarmos ao valor da derivada parcial do custo em função de cada parâmetro $w$ fazemos um processo similar ao utilizado em **Redes Neurais**. Porém, no caso da Regressão Logística, temos um único neurônio em uma única camada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de344c7",
   "metadata": {},
   "source": [
    "**Processo para chegarmos ao custo:**\n",
    "\n",
    "1. Calculamos o produto interno $$z_i=\\mathbf{w}^T\\mathbf{x}_i$$\n",
    "2. Calculamos a função de ativação $$a_i=\\theta(z_i)$$\n",
    "3. Perda individual: $$ L(a_i, y_i) = -[y_i\\times\\log{a_i + (1-y_i)\\times\\log{(1-a_i})}]$$\n",
    "4. Custo total: $$ J = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}(a_i, y_i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234ea18",
   "metadata": {},
   "source": [
    "**Processo para chegarmos às derivadas parciais:**\n",
    "\n",
    "1. Derivada parcial da perda em função de $a_i$: \n",
    "$$da_i = \\frac{\\partial \\mathcal{L}}{\\partial a_i} =  -\\frac{y_i}{a_i} + \\frac{1-y_i}{1-a_i}$$\n",
    "\n",
    "2. Derivada parcial da perda em função de $z_i$: \n",
    "$$dz=\\frac{\\partial \\mathcal{L}}{\\partial z_i} = \\frac{\\partial \\mathcal{L}}{\\partial a_i} \\times \\frac{\\partial a_i}{\\partial z_i} = a_i - y_i$$\n",
    "\n",
    "\n",
    "2. Derivada parcial da perda em função de $w_i$: \n",
    "$$dw=\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial w_j} = x_{ij}\\times dz_i $$\n",
    "\n",
    "\n",
    "4. **Considerando agora o custo total:**\n",
    "$$ \\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\\times dz_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491ddda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef1e0428",
   "metadata": {},
   "source": [
    "### Implementando o Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00878db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b8de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f5474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae901bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7476-b95c-4a92-9b8b-6652435a97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))    # compute cost\n",
    "    \n",
    "    dw = (1/m)*np.dot(X, (A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b104bb1-88dd-4ed0-bf52-08c17e41eb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdacb7-98a1-4b72-822b-18e748828723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1698d-746f-435a-975c-3a3223ef6444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6416b-8e34-4bc8-a57f-1399015dfb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe490bd-ea98-48c2-b702-7d6c3ac0ebc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dca70-9e46-4b4e-9e00-862e1410df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336378ea-3ff1-4a37-83e8-5e89a140cc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49327988-ee95-46f1-b6e3-99ed4415a509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3368ec1-72b9-4c8b-a6da-20e0481c85e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd13e8-0016-480a-a23d-05f5f27e3d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b6005-e86a-47b0-8944-fa3dfbe6059a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558193cb-6042-4808-907c-68a6ce4a9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de893d-f352-47ec-9767-6a07958cda36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
